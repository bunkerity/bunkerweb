# Utilisations avanc√©es

De nombreux exemples de cas d'utilisation concrets sont disponibles dans le dossier [examples](https://github.com/bunkerity/bunkerweb/tree/v1.6.6/examples) du d√©p√¥t GitHub.

Nous fournissons √©galement de nombreux mod√®les standard, tels que des fichiers YAML pour diverses int√©grations et types de bases de donn√©es. Ceux-ci sont disponibles dans le dossier [misc/integrations](https://github.com/bunkerity/bunkerweb/tree/v1.6.6/misc/integrations).

Cette section se concentre uniquement sur les utilisations avanc√©es et le r√©glage de la s√©curit√©, consultez la [section fonctionnalit√©s](features.md) de la documentation pour voir tous les param√®tres disponibles.

!!! tip "Tester"
    Pour effectuer des tests rapides lorsque le mode multisite est activ√© (et si vous n'avez pas les bonnes entr√©es DNS configur√©es pour les domaines), vous pouvez utiliser curl avec l'en-t√™te HTTP Host de votre choix :
    ```shell
    curl -H "Host: app1.example.com" http://ip-or-fqdn-of-server
    ```

    Si vous utilisez HTTPS, vous devrez configurer le SNI :
    ```shell
    curl -H "Host: app1.example.com" --resolve example.com:443:ip-of-server https://example.com
    ```

## Derri√®re l'√©quilibreur de charge ou le proxy inverse {#behind-load-balancer-or-reverse-proxy}

!!! info "Real IP"

    Lorsque BunkerWeb se trouve lui‚Äëm√™me derri√®re un √©quilibreur de charge ou un proxy inverse, vous devez le configurer afin qu'il puisse r√©cup√©rer la v√©ritable adresse IP des clients. **Si vous ne le faites pas, les fonctionnalit√©s de s√©curit√© bloqueront l'adresse IP de l'√©quilibreur de charge ou du proxy inverse au lieu de celle du client.**

BunkerWeb prend en fait en charge deux m√©thodes pour r√©cup√©rer l'adresse IP r√©elle du client :

- √Ä l'aide du `PROXY protocol`
- √Ä l'aide d'un en-t√™te HTTP tel que `X-Forwarded-For`

Les param√®tres suivants peuvent √™tre utilis√©s :

- `USE_REAL_IP` : activer/d√©sactiver la r√©cup√©ration d'IP r√©elle
- `USE_PROXY_PROTOCOL` : activer/d√©sactiver la prise en charge du protocole PROXY.
- `REAL_IP_FROM` : liste d'adresses IP/r√©seau de confiance autoris√©es pour nous envoyer la "vraie IP"
- `REAL_IP_HEADER` : l'en-t√™te HTTP contenant l'IP r√©elle ou la valeur sp√©ciale `proxy_protocol` lors de l'utilisation du protocole PROXY

Vous trouverez plus de param√®tres sur l'IP r√©elle dans la [section des fonctionnalit√©s](features.md#real-ip) de la documentation.

=== "En-t√™te HTTP"

    Nous supposerons ce qui suit concernant les √©quilibreurs de charge ou les proxies inverses (vous devrez mettre √† jour les param√®tres en fonction de votre configuration) :

    - Ils utilisent l'en-t√™te `X-Forwarded-For` pour d√©finir l'adresse IP r√©elle
    - Ils ont des adresses IP dans les r√©seaux `1.2.3.0/24` et `100.64.0.0/10`

    === "Interface utilisateur Web"

        Acc√©dez √† la page **Config Globale**, s√©lectionnez le plugin **Real IP** et renseignez les param√®tres suivants :

        <figure markdown>![Param√®tres Real IP (en-t√™te) via l'interface Web](assets/img/advanced-proxy1.png){ align=center }<figcaption>Param√®tres Real IP (en-t√™te) via l'interface Web</figcaption></figure>

        Veuillez noter qu'il est recommand√© de red√©marrer BunkerWeb lorsque vous modifiez des param√®tres li√©s √† la r√©cup√©ration de la vraie adresse IP.

    === "Linux"

        Vous devrez ajouter ces param√®tres au fichier /etc/bunkerweb/variables.env :

        ```conf
        ...
        USE_REAL_IP=yes
        REAL_IP_FROM=1.2.3.0/24 100.64.0.0/16
        REAL_IP_HEADER=X-Forwarded-For
        ...
        ```

        Veuillez noter qu'il est recommand√© de red√©marrer plut√¥t que de recharger le service lorsque vous modifiez les param√®tres li√©s √† la r√©cup√©ration de la vraie adresse IP :

        ```shell
        sudo systemctl restart bunkerweb && \
        sudo systemctl restart bunkerweb-scheduler
        ```

    === "Tout-en-un"

        Vous devrez ajouter ces param√®tres aux variables d'environnement lors de l'ex√©cution du conteneur All-in-one :

        ```bash
        docker run -d \
            --name bunkerweb-aio \
            -v bw-storage:/data \
            -e USE_REAL_IP="yes" \
            -e REAL_IP_FROM="1.2.3.0/24 100.64.0.0/10" \
            -e REAL_IP_HEADER="X-Forwarded-For" \
            -p 80:8080/tcp \
            -p 443:8443/tcp \
            -p 443:8443/udp \
            bunkerity/bunkerweb-all-in-one:1.6.6
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Docker"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des conteneurs BunkerWeb et du Scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Docker autoconf"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des conteneurs BunkerWeb et du Scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Kubernetes"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des pods BunkerWeb et du Scheduler.

        Voici la partie correspondante de votre fichier `values.yaml` que vous pouvez utiliser :

        ```yaml
        bunkerweb:
          extraEnvs:
            - name: USE_REAL_IP
              value: "yes"
            - name: REAL_IP_FROM
              value: "1.2.3.0/24 100.64.0.0/10"
            - name: REAL_IP_HEADER
              value: "X-Forwarded-For"
        scheduler:
          extraEnvs:
            - name: USE_REAL_IP
              value: "yes"
            - name: REAL_IP_FROM
              value: "1.2.3.0/24 100.64.0.0/10"
            - name: REAL_IP_HEADER
              value: "X-Forwarded-For"
        ```

    === "Swarm"

        !!! warning "Obsol√®te"
            L'int√©gration Swarm est obsol√®te et sera supprim√©e dans une future version. Veuillez envisager d'utiliser l'[int√©gration Kubernetes](integrations.md#kubernetes) √† la place.

            **Plus d'informations sont disponibles dans la [documentation de l'int√©gration Swarm](integrations.md#swarm).**

        Vous devrez ajouter ces param√®tres aux variables d'environnement des services BunkerWeb et scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "X-Forwarded-For"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

=== "Protocole proxy"

    !!! warning "Lire attentivement"

      N'utilisez le protocole `PROXY protocol` que si vous √™tes certain que votre √©quilibreur de charge ou proxy inverse l'envoie. **Si vous l'activez et qu'il n'est pas utilis√©, vous obtiendrez des erreurs**.

    Nous supposerons ce qui suit concernant les √©quilibreurs de charge ou les proxies inverses (vous devrez adapter les param√®tres en fonction de votre configuration) :

    - Ils utilisent le `PROXY protocol` v1 ou v2 pour d√©finir l'adresse IP r√©elle
    - Ils ont des adresses IP dans les r√©seaux `1.2.3.0/24` et `100.64.0.0/10`

    === "Interface utilisateur Web"

        Acc√©dez √† la page **Config Globale**, s√©lectionnez le plugin **Real IP** et renseignez les param√®tres suivants :

        <figure markdown>![Param√®tres Real IP (protocole PROXY) via l'interface Web](assets/img/advanced-proxy2.png){ align=center }<figcaption>Param√®tres Real IP (protocole PROXY) via l'interface Web</figcaption></figure>

        Veuillez noter qu'il est recommand√© de red√©marrer BunkerWeb lorsque vous modifiez des param√®tres li√©s √† la r√©cup√©ration de la vraie adresse IP.

    === "Linux"

        Vous devrez ajouter ces param√®tres au fichier /etc/bunkerweb/variables.env :

        ```conf
        ...
        USE_REAL_IP=yes
        USE_PROXY_PROTOCOL=yes
        REAL_IP_FROM=1.2.3.0/24 100.64.0.0/16
        REAL_IP_HEADER=proxy_protocol
        ...
        ```

        Veuillez noter qu'il est recommand√© de red√©marrer plut√¥t que de recharger le service lors de la configuration des param√®tres li√©s au protocole PROXY :

        ```shell
        sudo systemctl restart bunkerweb && \
        sudo systemctl restart bunkerweb-scheduler
        ```

    === "Tout-en-un"

        Vous devrez ajouter ces param√®tres aux variables d'environnement lors de l'ex√©cution du conteneur All-in-one :

        ```bash
        docker run -d \
            --name bunkerweb-aio \
            -v bw-storage:/data \
            -e USE_REAL_IP="yes" \
            -e USE_PROXY_PROTOCOL="yes" \
            -e REAL_IP_FROM="1.2.3.0/24 100.64.0.0/10" \
            -e REAL_IP_HEADER="X-Forwarded-For" \
            -p 80:8080/tcp \
            -p 443:8443/tcp \
            -p 443:8443/udp \
            bunkerity/bunkerweb-all-in-one:1.6.6
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Docker"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des conteneurs BunkerWeb et du Scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Docker autoconf"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des conteneurs BunkerWeb et du Scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    === "Kubernetes"

        Vous devrez ajouter ces param√®tres aux variables d'environnement des pods BunkerWeb et du Scheduler.

        Voici la partie correspondante de votre fichier `values.yaml` que vous pouvez utiliser :

        ```yaml
        bunkerweb:
          extraEnvs:
            - name: USE_REAL_IP
              value: "yes"
            - name: USE_PROXY_PROTOCOL
              value: "yes"
            - name: REAL_IP_FROM
              value: "1.2.3.0/24 100.64.0.0/10"
            - name: REAL_IP_HEADER
              value: "proxy_protocol"
        scheduler:
          extraEnvs:
            - name: USE_REAL_IP
              value: "yes"
            - name: USE_PROXY_PROTOCOL
              value: "yes"
            - name: REAL_IP_FROM
              value: "1.2.3.0/24 100.64.0.0/10"
            - name: REAL_IP_HEADER
              value: "proxy_protocol"
        ```

    === "Swarm"

        !!! warning "Obsol√®te"
            L'int√©gration Swarm est obsol√®te et sera supprim√©e dans une future version. Veuillez envisager d'utiliser l'[int√©gration Kubernetes](integrations.md#kubernetes) √† la place.

            **Plus d'informations sont disponibles dans la [documentation de l'int√©gration Swarm](integrations.md#swarm).**

        Vous devrez ajouter ces param√®tres aux variables d'environnement des services BunkerWeb et scheduler :

        ```yaml
        bunkerweb:
          image: bunkerity/bunkerweb:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ...
        bw-scheduler:
          image: bunkerity/bunkerweb-scheduler:1.6.6
          ...
          environment:
            USE_REAL_IP: "yes"
            USE_PROXY_PROTOCOL: "yes"
            REAL_IP_FROM: "1.2.3.0/24 100.64.0.0/10"
            REAL_IP_HEADER: "proxy_protocol"
          ...
        ```

        Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

## Haute disponibilit√© et r√©partition de charge

Pour garantir la disponibilit√© de vos applications m√™me si un serveur tombe, vous pouvez d√©ployer BunkerWeb en cluster **HA**. Cette architecture comporte un **Manager** (Scheduler) qui orchestre la configuration et plusieurs **Workers** (instances BunkerWeb) qui traitent le trafic.

```mermaid
flowchart LR

  %% ================ Styles =================
  classDef manager     fill:#eef2ff,stroke:#4c1d95,stroke-width:1px,rx:6px,ry:6px;
  classDef component     fill:#f9fafb,stroke:#6b7280,stroke-width:1px,rx:4px,ry:4px;
  classDef lb            fill:#e0f2fe,stroke:#0369a1,stroke-width:1px,rx:6px,ry:6px;
  classDef database fill:#d1fae5,stroke:#059669,stroke-width:1px,rx:4px,ry:4px;
  classDef datastore     fill:#fee2e2,stroke:#b91c1c,stroke-width:1px,rx:4px,ry:4px;
  classDef backend       fill:#ede9fe,stroke:#7c3aed,stroke-width:1px,rx:4px,ry:4px;
  classDef client        fill:#e5e7eb,stroke:#4b5563,stroke-width:1px,rx:4px,ry:4px;

  %% Container styles
  style CLUSTER fill:#f3f4f6,stroke:#d1d5db,stroke-width:1px,stroke-dasharray:6 3;
  style WORKERS fill:none,stroke:#9ca3af,stroke-width:1px,stroke-dasharray:4 2;

  %% ============== Outside left =============
  Client["Client"]:::client
  LB["Load Balancer"]:::lb

  %% ============== Cluster ==================
  subgraph CLUSTER[" "]
    direction TB

    %% ---- Top row: Manager + Redis ----
    subgraph TOP["Manager & Data Stores"]
      direction LR
      Manager["Manager<br/>(Scheduler)"]:::manager
      BDD["BDD"]:::database
      Redis["Redis/Valkey"]:::datastore
      UI["Interface Web"]:::manager
    end

    %% ---- Middle: Workers ----
    subgraph WORKERS["Workers (BunkerWeb)"]
      direction TB
      Worker1["Worker 1"]:::component
      WorkerN["Worker N"]:::component
    end

    %% ---- Bottom: App ----
    App["App"]:::backend
  end

  %% ============ Outside right ============
  Admin["Admin"]:::client

  %% ============ Traffic & control ===========
  %% Manager / control plane
  Manager -->|API 5000| Worker1
  Manager -->|API 5000| WorkerN
  Manager -->|bwcli| Redis
  Manager -->|Config| BDD

  %% User interface (UI)
  UI -->|Config| BDD
  UI -->|Reports / Bans| Redis
  BDD --- UI
  Redis --- UI
  linkStyle 6 stroke-width:0px;
  linkStyle 7 stroke-width:0px;

  %% Workers <-> Redis
  Worker1 -->|Cache partag√©| Redis
  WorkerN -->|Cache partag√©| Redis

  %% Workers -> App
  Worker1 -->|Trafic l√©gitime| App
  WorkerN -->|Trafic l√©gitime| App

  %% Client (right side) -> Load balancer -> Workers -> App
  Client -->|Requ√™te| LB
  LB -->|HTTP/TCP| Worker1
  LB -->|HTTP/TCP| WorkerN

  %% Admin -> UI
  UI --- Admin
  Admin -->|HTTP| UI
  linkStyle 15 stroke-width:0px;
```

!!! info "Comprendre les API BunkerWeb"
    BunkerWeb s'appuie sur deux notions d'API diff√©rentes :

    - Une **API interne** qui connecte automatiquement managers et workers pour l'orchestration. Elle est toujours activ√©e et ne n√©cessite aucune configuration manuelle.
    - Un **service API** optionnel (`bunkerweb-api`) qui expose une interface REST publique pour les outils d'automatisation (bwcli, CI/CD, etc.). Il est d√©sactiv√© par d√©faut sur les installations Linux et est ind√©pendant des communications internes manager‚Üîworker.

### Pr√©requis

Avant de mettre en place un cluster, assurez-vous de disposer de :

- **Au moins 2 h√¥tes Linux** avec acc√®s root/sudo.
- **Connectivit√© r√©seau** entre les h√¥tes (en particulier sur le port TCP 5000 pour l'API interne).
- **L'IP ou le nom d'h√¥te** de l'application √† prot√©ger.
- *(Optionnel)* **√âquilibreur de charge** (par ex. HAProxy) pour r√©partir le trafic entre les workers.

### 1. Installer le Manager

Le Manager est le cerveau du cluster. Il ex√©cute le Scheduler, la base de donn√©es et, optionnellement, l'interface Web.

!!! warning "S√©curit√© de l'interface Web"
    L'interface Web √©coute sur un port d√©di√© (7000 par d√©faut) et ne doit √™tre accessible qu'aux administrateurs. Si vous pr√©voyez de l'exposer √† Internet, nous **recommandons fortement** de la prot√©ger avec une instance BunkerWeb en frontal.

=== "Linux"

    1. **T√©l√©charger et lancer l'installateur** sur l'h√¥te manager :

        ```bash
        # T√©l√©charger le script et sa somme
        curl -fsSL -O https://github.com/bunkerity/bunkerweb/releases/download/v1.6.6/install-bunkerweb.sh
        curl -fsSL -O https://github.com/bunkerity/bunkerweb/releases/download/v1.6.6/install-bunkerweb.sh.sha256

        # V√©rifier l'empreinte
        sha256sum -c install-bunkerweb.sh.sha256

        # Ex√©cuter l'installateur
        chmod +x install-bunkerweb.sh
        sudo ./install-bunkerweb.sh
        ```

        !!! danger "Avis de s√©curit√©"
            V√©rifiez toujours l'int√©grit√© du script avec la somme fournie avant de l'ex√©cuter.

    2. **Choisissez l'option 2) Manager** et suivez les invites :

        | Invite                     | Action                                                                                        |
        | :------------------------- | :-------------------------------------------------------------------------------------------- |
        | **Instances BunkerWeb**    | Saisissez les IP de vos n≈ìuds worker s√©par√©es par des espaces (ex : `192.168.10.11 192.168.10.12`). |
        | **Whitelist IP**           | Acceptez l'IP d√©tect√©e ou saisissez un sous-r√©seau (ex : `192.168.10.0/24`) pour autoriser l'acc√®s √† l'API. |
        | **R√©solveurs DNS**         | Appuyez sur `N` pour la valeur par d√©faut ou fournissez les v√¥tres.                           |
        | **HTTPS pour l'API interne** | **Recommand√© :** `Y` pour g√©n√©rer automatiquement des certificats et s√©curiser les √©changes manager-worker. |
        | **Service Web UI**         | `Y` pour activer l'interface Web (fortement recommand√©).                                      |
        | **Service API**            | `N` sauf besoin d'API REST publique pour des outils externes.                                 |

    #### S√©curiser et exposer l'UI

    Si vous avez activ√© l'interface Web, vous devez la s√©curiser. Elle peut √™tre h√©berg√©e sur le Manager ou une machine d√©di√©e.

    === "H√©berg√©e sur le Manager"

        1. √âditez `/etc/bunkerweb/ui.env` pour d√©finir des identifiants forts :

        ```ini
        # OVERRIDE_ADMIN_CREDS=no
        ADMIN_USERNAME=admin
        ADMIN_PASSWORD=changeme
        # FLASK_SECRET=changeme
        # TOTP_ENCRYPTION_KEYS=changeme
        LISTEN_ADDR=0.0.0.0
        # LISTEN_PORT=7000
        FORWARDED_ALLOW_IPS=127.0.0.1
        # ENABLE_HEALTHCHECK=no
        ```

        !!! warning "Changer les identifiants par d√©faut"
            Remplacez `admin` et `changeme` par des identifiants forts avant de d√©marrer le service UI en production.

        2. Red√©marrez l'UI :

        ```bash
        sudo systemctl restart bunkerweb-ui
        ```

    === "H√¥te externe"

        Pour plus d'isolation, installez l'UI sur un n≈ìud s√©par√©.

        1. Lancez l'installateur et choisissez **Option 5) Web UI Only**.
        2. √âditez `/etc/bunkerweb/ui.env` pour pointer vers la base du Manager :

            ```ini
            # Configuration base de donn√©es (doit correspondre √† celle du Manager)
            DATABASE_URI=mariadb+pymysql://bunkerweb:changeme@db-host:3306/bunkerweb
            # Pour PostgreSQL : postgresql://bunkerweb:changeme@db-host:5432/bunkerweb
            # Pour MySQL : mysql+pymysql://bunkerweb:changeme@db-host:3306/bunkerweb

            # Configuration Redis (si Redis/Valkey est utilis√© pour la persistance)
            # Si non fourni, il est automatiquement pris depuis la base de donn√©es
            # REDIS_HOST=redis-host

            # Identifiants de s√©curit√©
            ADMIN_USERNAME=admin
            ADMIN_PASSWORD=changeme

            # R√©glages r√©seau
            LISTEN_ADDR=0.0.0.0
            # LISTEN_PORT=7000
            ```

        3. Red√©marrez le service :

            ```bash
            sudo systemctl restart bunkerweb-ui
            ```

        !!! tip "Configuration du pare-feu"
            Assurez-vous que l'h√¥te UI peut joindre la base et Redis. Vous devrez peut-√™tre ajuster les r√®gles sur l'h√¥te UI ainsi que sur les h√¥tes base/Redis.

=== "Docker"

    Cr√©ez un fichier `docker-compose.yml` sur l'h√¥te manager :

    ```yaml title="docker-compose.yml"
    x-ui-env: &bw-ui-env
      # Nous ancrons les variables d'environnement pour √©viter les duplications
      DATABASE_URI: "mariadb+pymysql://bunkerweb:changeme@bw-db:3306/db" # Pensez √† mettre un mot de passe plus fort

    services:
      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          <<: *bw-ui-env
          BUNKERWEB_INSTANCES: "192.168.1.11 192.168.1.12" # Remplacez par les IPs de vos workers
          API_WHITELIST_IP: "127.0.0.0/8 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16" # Autoriser les r√©seaux locaux
          # API_LISTEN_HTTPS: "yes" # Recommand√© pour s√©curiser l'API interne
          # API_TOKEN: "my_secure_token" # Optionnel : d√©finir un token suppl√©mentaire
          SERVER_NAME: ""
          MULTISITE: "yes"
          USE_REDIS: "yes"
          REDIS_HOST: "redis"
        volumes:
          - bw-storage:/data # Persistance du cache et des sauvegardes
        restart: "unless-stopped"
        networks:
          - bw-db
          - bw-redis

      bw-ui:
        image: bunkerity/bunkerweb-ui:1.6.6
        ports:
          - "7000:7000" # Exposer le port de l'UI
        environment:
          <<: *bw-ui-env
          ADMIN_USERNAME: "changeme"
          ADMIN_PASSWORD: "changeme" # Remplacez par un mot de passe plus fort
          TOTP_ENCRYPTION_KEYS: "mysecret" # Remplacez par une cl√© plus forte (voir la section Pr√©requis)
        restart: "unless-stopped"
        networks:
          - bw-db
          - bw-redis

      bw-db:
        image: mariadb:11
        # Nous fixons la taille max des paquets pour √©viter les soucis de grosses requ√™tes
        command: --max-allowed-packet=67108864
        environment:
          MYSQL_RANDOM_ROOT_PASSWORD: "yes"
          MYSQL_DATABASE: "db"
          MYSQL_USER: "bunkerweb"
          MYSQL_PASSWORD: "changeme" # Remplacez par un mot de passe plus fort
        volumes:
          - bw-data:/var/lib/mysql
        restart: "unless-stopped"
        networks:
          - bw-db

      redis: # Redis pour la persistance des rapports/bannissements/stats
        image: redis:7-alpine
        command: >
          redis-server
          --maxmemory 256mb
          --maxmemory-policy allkeys-lru
          --save 60 1000
          --appendonly yes
        volumes:
          - redis-data:/data
        restart: "unless-stopped"
        networks:
          - bw-redis

    volumes:
      bw-data:
      bw-storage:
      redis-data:

    networks:
      bw-db:
        name: bw-db
      bw-redis:
        name: bw-redis
    ```

    D√©marrez la pile manager :

    ```bash
    docker compose up -d
    ```

### 2. Installer les Workers

Les workers sont les n≈ìuds qui traitent le trafic entrant.

=== "Linux"

    1. **Lancez l'installateur** sur chaque worker (m√™mes commandes que pour le Manager).
    2. **Choisissez l'option 3) Worker** et r√©pondez :

        | Invite                     | Action                                                     |
        | :------------------------- | :--------------------------------------------------------- |
        | **IP du Manager**          | Saisissez l'IP du Manager (ex : `192.168.10.10`).          |
        | **HTTPS pour l'API interne** | Doit correspondre au choix du Manager (`Y` ou `N`).        |

    Le worker s'enregistrera automatiquement aupr√®s du Manager.

=== "Docker"

    Cr√©ez un fichier `docker-compose.yml` sur chaque worker :

    ```yaml title="docker-compose.yml"
    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ports:
          - "80:8080/tcp"
          - "443:8443/tcp"
          - "443:8443/udp" # Support QUIC / HTTP3
          - "5000:5000/tcp" # Port de l'API interne
        environment:
          API_WHITELIST_IP: "127.0.0.0/8 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16"
          # API_LISTEN_HTTPS: "yes" # Recommand√© pour s√©curiser l'API interne (doit correspondre au Manager)
          # API_TOKEN: "my_secure_token" # Optionnel : token suppl√©mentaire (doit correspondre au Manager)
        restart: "unless-stopped"
    ```

    D√©marrez le worker :

    ```bash
    docker compose up -d
    ```

### 3. G√©rer les Workers

Vous pouvez ajouter d'autres workers plus tard via l'interface Web ou la CLI.

=== "Via l'interface Web"

    1. **Ouvrez l'onglet Instances**.
    2. **Cliquez sur Add instance**.
    3. **Renseignez l'IP/hostname du worker** puis enregistrez.

    <div class="grid grid-2" markdown style="display:grid; align-items:center;">
    <figure markdown style="display:flex; flex-direction:column; justify-content:center; align-items:center; height:100%;">
      ![BunkerWeb UI - Cr√©ation d'instance](assets/img/ui-ha-create-instance.webp){ width="100%" }
      <figcaption>BunkerWeb UI - Cr√©ation d'instance</figcaption>
    </figure>
    <figure markdown style="display:flex; flex-direction:column; justify-content:center; align-items:center; height:100%;">
      ![BunkerWeb UI - Formulaire de cr√©ation](assets/img/ui-ha-create-instance-form.webp){ width="100%" }
      <figcaption>BunkerWeb UI - Formulaire de cr√©ation</figcaption>
    </figure>
    </div>

=== "Via la configuration"

    === "Linux"

        1. **Modifiez** `/etc/bunkerweb/variables.env` sur le Manager :

            ```bash
            BUNKERWEB_INSTANCES=192.168.10.11 192.168.10.12 192.168.10.13
            ```

        2. **Red√©marrez le Scheduler** :

            ```bash
            sudo systemctl restart bunkerweb-scheduler
            ```

    === "Docker"

        1. **Modifiez** le fichier `docker-compose.yml` sur le Manager pour mettre √† jour `BUNKERWEB_INSTANCES`.

        2. **Recr√©ez le conteneur du Scheduler** :

            ```bash
            docker compose up -d bw-scheduler
            ```

### 4. V√©rifier l'installation

=== "Linux"

    1. **V√©rifier le statut** : connectez-vous √† l'UI (`http://<ip-manager>:7000`) et ouvrez l'onglet **Instances**. Tous les workers doivent √™tre **Up**.
    2. **Tester le basculement** : arr√™tez BunkerWeb sur un worker (`sudo systemctl stop bunkerweb`) et v√©rifiez que le trafic continue de passer.

=== "Docker"

    1. **V√©rifier le statut** : connectez-vous √† l'UI (`http://<ip-manager>:7000`) et ouvrez l'onglet **Instances**. Tous les workers doivent √™tre **Up**.
    2. **Tester le basculement** : arr√™tez BunkerWeb sur un worker (`docker compose stop bunkerweb`) et v√©rifiez que le trafic continue de passer.

### 5. R√©partition de charge

Pour r√©partir le trafic entre vos workers, utilisez un √©quilibreur de charge. Nous recommandons un √©quilibreur de couche 4 (TCP) qui supporte le **PROXY protocol** pour pr√©server l'IP client.

=== "HAProxy - Couche 4 (TCP)"

    Exemple de configuration **HAProxy** qui passe le trafic (mode TCP) tout en conservant l'IP client via le **PROXY protocol**.

    ```cfg title="haproxy.cfg"
    defaults
        timeout connect 5s
        timeout client 5s
        timeout server 5s

    frontend http_front
        mode tcp
        bind *:80
        default_backend http_back

    frontend https_front
        mode tcp
        bind *:443
        default_backend https_back

    backend http_back
        mode tcp
        balance roundrobin
        server worker01 192.168.10.11:80 check send-proxy-v2
        server worker02 192.168.10.12:80 check send-proxy-v2

    backend https_back
        mode tcp
        balance roundrobin
        server worker01 192.168.10.11:443 check send-proxy-v2
        server worker02 192.168.10.12:443 check send-proxy-v2
    ```

=== "HAProxy - Couche 7 (HTTP)"

    Exemple de configuration **HAProxy** pour la r√©partition en couche 7 (HTTP). Elle ajoute l'en-t√™te `X-Forwarded-For` pour que BunkerWeb r√©cup√®re l'IP client.

    ```cfg title="haproxy.cfg"
    defaults
        timeout connect 5s
        timeout client 5s
        timeout server 5s

    frontend http_front
        mode http
        bind *:80
        default_backend http_back

    frontend https_front
        mode http
        bind *:443
        default_backend https_back

    backend http_back
        mode http
        balance roundrobin
        option forwardfor
        server worker01 192.168.10.11:80 check
        server worker02 192.168.10.12:80 check

    backend https_back
        mode http
        balance roundrobin
        option forwardfor
        server worker01 192.168.10.11:443 check
        server worker02 192.168.10.12:443 check
    ```

Rechargez HAProxy une fois la configuration enregistr√©e :

```bash
sudo systemctl restart haproxy
```

Pour plus d'informations, consultez la [documentation officielle HAProxy](http://docs.haproxy.org/).

!!! tip "Configurer l'IP r√©elle"
    N'oubliez pas de configurer BunkerWeb pour r√©cup√©rer la v√©ritable IP client (via PROXY protocol ou l'en-t√™te X-Forwarded-For).

    Reportez-vous √† la section [Derri√®re l'√©quilibreur de charge ou le proxy inverse](#behind-load-balancer-or-reverse-proxy) pour v√©rifier que vous utilisez la bonne IP client.

    Consultez `/var/log/bunkerweb/access.log` sur chaque worker pour confirmer que les requ√™tes proviennent du r√©seau PROXY protocol et que les deux workers se partagent la charge. Votre cluster BunkerWeb est maintenant pr√™t pour la production avec haute disponibilit√©.

## Utilisation de m√©canismes de r√©solution DNS personnalis√©s

La configuration NGINX de BunkerWeb peut √™tre personnalis√©e pour utiliser diff√©rents r√©solveurs DNS en fonction de vos besoins. Cela peut √™tre particuli√®rement utile dans divers sc√©narios :

1. Pour respecter les entr√©es de votre `/etc/hosts` fichier local
2. Lorsque vous devez utiliser des serveurs DNS personnalis√©s pour certains domaines
3. Pour s'int√©grer √† des solutions de mise en cache DNS locales

### Utilisation de systemd-resolved

De nombreux syst√®mes Linux modernes utilisent `systemd-resolved` la r√©solution DNS. Si vous souhaitez que BunkerWeb respecte le contenu de votre `/etc/hosts` fichier et utilise le m√©canisme de r√©solution DNS du syst√®me, vous pouvez le configurer pour utiliser le service DNS local r√©solu par systemd.

Pour v√©rifier que systemd-resolved est en cours d'ex√©cution sur votre syst√®me, vous pouvez utiliser :

```bash
systemctl status systemd-resolved
```

Pour activer systemd-resolved comme r√©solveur DNS dans BunkerWeb, d√©finissez le `DNS_RESOLVERS` param√®tre sur `127.0.0.53`, qui est l'adresse d'√©coute par d√©faut pour systemd-resolved :

=== "Interface utilisateur Web"

    Acc√©dez √† la page **Config Globale** et d√©finissez les r√©solveurs DNS sur `127.0.0.53`

    <figure markdown>![Param√®tres des r√©solveurs DNS via l'interface Web](assets/img/advanced-dns-resolvers.png){ align=center }<figcaption>Param√®tres des r√©solveurs DNS via l'interface Web</figcaption></figure>

=== "Linux"

    Vous devrez modifier le fichier /etc/bunkerweb/variables.env :

    ```conf
    ...
    DNS_RESOLVERS=127.0.0.53
    ...
    ```

    Apr√®s avoir effectu√© cette modification, rechargez le Scheduler pour appliquer la configuration :

    ```shell
    sudo systemctl reload bunkerweb-scheduler
    ```

### Utilisation de dnsmasq

[dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html) est un serveur DNS, DHCP et TFTP l√©ger qui est couramment utilis√© pour la mise en cache et la personnalisation du DNS local. C'est particuli√®rement utile lorsque vous avez besoin de plus de contr√¥le sur votre r√©solution DNS que celui fourni par systemd-resolved.

=== "Linux"

    Tout d'abord, installez et configurez dnsmasq sur votre syst√®me Linux¬†:

    === "Debian/Ubuntu"

        ```bash
        # Install dnsmasq
        sudo apt-get update && sudo apt-get install dnsmasq

        # Configure dnsmasq to listen only on localhost
        echo "listen-address=127.0.0.1" | sudo tee -a /etc/dnsmasq.conf
        echo "bind-interfaces" | sudo tee -a /etc/dnsmasq.conf

        # Add custom DNS entries if needed
        echo "address=/custom.example.com/192.168.1.10" | sudo tee -a /etc/dnsmasq.conf

        # Restart dnsmasq
        sudo systemctl restart dnsmasq
        sudo systemctl enable dnsmasq
        ```

    === "RHEL/Fedora"

        ```bash
        # Install dnsmasq
        sudo dnf install dnsmasq

        # Configure dnsmasq to listen only on localhost
        echo "listen-address=127.0.0.1" | sudo tee -a /etc/dnsmasq.conf
        echo "bind-interfaces" | sudo tee -a /etc/dnsmasq.conf

        # Add custom DNS entries if needed
        echo "address=/custom.example.com/192.168.1.10" | sudo tee -a /etc/dnsmasq.conf

        # Restart dnsmasq
        sudo systemctl restart dnsmasq
        sudo systemctl enable dnsmasq
        ```

    Ensuite, configurez BunkerWeb pour utiliser dnsmasq en d√©finissant `DNS_RESOLVERS` sur `127.0.0.1` :

    === "Web UI"

        Acc√©dez √† la page **Config Globale** et s√©lectionnez le plugin **NGINX**, puis d√©finissez les r√©solveurs DNS sur `127.0.0.1`.

        <figure markdown>![Param√®tres des r√©solveurs DNS via l'interface Web](assets/img/advanced-dns-resolvers2.png){ align=center }<figcaption>Param√®tres des r√©solveurs DNS via l'interface Web</figcaption></figure>

    === "Linux"

        Vous devrez modifier le fichier `/etc/bunkerweb/variables.env` :

        ```conf
        ...
        DNS_RESOLVERS=127.0.0.1
        ...
        ```

        Apr√®s avoir effectu√© cette modification, rechargez le Scheduler pour appliquer la configuration :

        ```shell
        sudo systemctl reload bunkerweb-scheduler
        ```

=== "Tout-en-un"

    Lorsque vous utilisez l'image All-in-one, ex√©cutez dnsmasq dans un conteneur s√©par√© et configurez BunkerWeb pour l'utiliser :

    ```bash
    # Create a custom network for DNS communication
    docker network create bw-dns

    # Run dnsmasq container using dockurr/dnsmasq with Quad9 DNS
    # Quad9 provides security-focused DNS resolution with malware blocking
    docker run -d \
        --name dnsmasq \
        --network bw-dns \
        -e DNS1="9.9.9.9" \
        -e DNS2="149.112.112.112" \
        -p 53:53/udp \
        -p 53:53/tcp \
        --cap-add=NET_ADMIN \
        --restart=always \
        dockurr/dnsmasq

    # Run BunkerWeb All-in-one with dnsmasq DNS resolver
    docker run -d \
        --name bunkerweb-aio \
        --network bw-dns \
        -v bw-storage:/data \
        -e DNS_RESOLVERS="dnsmasq" \
        -p 80:8080/tcp \
        -p 443:8443/tcp \
        -p 443:8443/udp \
        bunkerity/bunkerweb-all-in-one:1.6.6
    ```

=== "Docker"

    Ajoutez un service dnsmasq √† votre fichier docker-compose et configurez BunkerWeb pour l'utiliser¬†:

    ```yaml
    services:
      dnsmasq:
        image: dockurr/dnsmasq
        container_name: dnsmasq
        environment:
          # Using Quad9 DNS servers for enhanced security and privacy
          # Primary: 9.9.9.9 (Quad9 with malware blocking)
          # Secondary: 149.112.112.112 (Quad9 backup server)
          DNS1: "9.9.9.9"
          DNS2: "149.112.112.112"
        ports:
          - 53:53/udp
          - 53:53/tcp
        cap_add:
          - NET_ADMIN
        restart: always
        networks:
          - bw-dns

      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ...
        environment:
          DNS_RESOLVERS: "dnsmasq"
        ...
        networks:
          - bw-universe
          - bw-services
          - bw-dns

      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        ...
        environment:
          DNS_RESOLVERS: "dnsmasq"
        ...
        networks:
          - bw-universe
          - bw-dns

    networks:
      # ...existing networks...
      bw-dns:
        name: bw-dns
    ```

## Configurations personnalis√©es {#custom-configurations}

Pour personnaliser et ajouter des configurations personnalis√©es √† BunkerWeb, vous pouvez profiter de sa base NGINX. Des configurations NGINX personnalis√©es peuvent √™tre ajout√©es dans diff√©rents contextes NGINX, y compris des configurations pour le pare-feu d'applications Web (WAF) ModSecurity, qui est un composant central de BunkerWeb. Vous trouverez plus de d√©tails sur les configurations de ModSecurity [ici](features.md#custom-configurations).

Voici les types de configurations personnalis√©es disponibles :

- **http** : Configurations au niveau HTTP de NGINX.
- **server-http** : configurations au niveau HTTP/Server de NGINX.
- **default-server-http**: configurations au niveau du serveur de NGINX, en particulier pour le "serveur par d√©faut" lorsque le nom de client fourni ne correspond √† aucun nom de serveur dans `SERVER_NAME`.
- **modsec-crs**: Configurations appliqu√©es avant le chargement de l'ensemble de r√®gles de base OWASP.
- **modsec**: configurations appliqu√©es apr√®s le chargement de l'ensemble de r√®gles de base OWASP ou utilis√©es lorsque l'ensemble de r√®gles de base n'est pas charg√©.
- **crs-plugins-before**: Configurations pour les plugins CRS, appliqu√©es avant le chargement des plugins CRS.
- **crs-plugins-after**: Configurations pour les plugins CRS, appliqu√©es apr√®s le chargement des plugins CRS.
- **stream** : Configurations au niveau du flux de NGINX.
- **server-stream** : Configurations au niveau Stream/Server de NGINX.

Les configurations personnalis√©es peuvent √™tre appliqu√©es globalement ou sp√©cifiquement pour un serveur particulier, en fonction du contexte applicable et de l'activation ou non du [mode multisite](features.md#multisite-mode) .

La m√©thode d'application des configurations personnalis√©es d√©pend de l'int√©gration utilis√©e. Cependant, le processus sous-jacent implique l'ajout de fichiers avec le `.conf` suffixe √† des dossiers sp√©cifiques. Pour appliquer une configuration personnalis√©e √† un serveur sp√©cifique, le fichier doit √™tre plac√© dans un sous-dossier nomm√© d'apr√®s le nom du serveur principal.

Certaines int√©grations offrent des moyens plus pratiques d'appliquer des configurations, par exemple √† l'aide de [Configs](https://docs.docker.com/engine/swarm/configs/) dans Docker Swarm ou de [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) dans Kubernetes. Ces options offrent des approches plus simples pour la gestion et l'application des configurations.

=== "Interface utilisateur Web"

    Acc√©dez √† la page **Configs**, cliquez sur **Create new custom config**, puis choisissez s'il s'agit d'une configuration globale ou sp√©cifique √† un service, le type de configuration et le nom de la configuration :

    <figure markdown>![Configurations personnalis√©es via l'interface Web](assets/img/advanced-config.png){ align=center }<figcaption>Configurations personnalis√©es via l'interface Web</figcaption></figure>

    N'oubliez pas de cliquer sur le bouton `üíæ Enregistrer`.

=== "Linux"

    Lorsque vous utilisez l'int√©gration [Linux](integrations.md#linux), les configurations personnalis√©es doivent √™tre √©crites dans le dossier `/etc/bunkerweb/configs`.

    Voici un exemple pour server-http/hello-world.conf :

    ```nginx
    location /hello {
      default_type 'text/plain';
      content_by_lua_block {
        ngx.say('world')
      }
    }
    ```

    Comme BunkerWeb s'ex√©cute en tant qu'utilisateur non privil√©gi√© (nginx:nginx), vous devrez modifier les permissions :

    ```shell
    chown -R root:nginx /etc/bunkerweb/configs && \
    chmod -R 770 /etc/bunkerweb/configs
    ```

    V√©rifions maintenant l'√©tat du Scheduler :

    ```shell
    systemctl status bunkerweb-scheduler
    ```

    S'ils sont d√©j√† en cours d'ex√©cution, nous pouvons le recharger :

    ```shell
    systemctl reload bunkerweb-scheduler
    ```

    Sinon, nous devrons le d√©marrer :

    ```shell
    systemctl start bunkerweb-scheduler
    ```

=== "Tout-en-un"

    Lorsque vous utilisez l'image [Tout-en-un](integrations.md#all-in-one-aio-image), vous avez deux options pour ajouter des configurations personnalis√©es :

    - Utilisation de param√®tres sp√©cifiques `*_CUSTOM_CONF_*` comme variables d'environnement lors de l'ex√©cution du conteneur (recommand√©).
    - √âcriture `.conf` de fichiers dans le `/data/configs/` r√©pertoire du volume mont√© sur `/data`.

    **Utilisation des param√®tres (variables d'environnement)**

    Les param√®tres √† utiliser doivent suivre le sch√©ma `<SITE>_CUSTOM_CONF_<TYPE>_<NAME>`:

    - `<SITE>` : Nom du serveur primaire facultatif si le mode multisite est activ√© et que la configuration doit √™tre appliqu√©e √† un service sp√©cifique.
    - `<TYPE>` : Le type de configuration, les valeurs accept√©es sont `HTTP`, `DEFAULT_SERVER_HTTP`, `SERVER_HTTP` `MODSEC` `MODSEC_CRS` `CRS_PLUGINS_BEFORE`, `CRS_PLUGINS_AFTER` `STREAM` , et `SERVER_STREAM`.
    - `<NAME>` : Le nom de la configuration sans le `.conf` suffixe.

    Voici un exemple fictif lors de l'ex√©cution du conteneur All-in-one :

    ```bash
    docker run -d \
        --name bunkerweb-aio \
        -v bw-storage:/data \
        -e "CUSTOM_CONF_SERVER_HTTP_hello-world=location /hello { \
            default_type 'text/plain'; \
            content_by_lua_block { \
              ngx.say('world'); \
            } \
          }" \
        -p 80:8080/tcp \
        -p 443:8443/tcp \
        bunkerity/bunkerweb-all-in-one:1.6.6
    ```

    Veuillez noter que si votre conteneur est d√©j√† cr√©√©, vous devrez le supprimer et le recr√©er pour que les nouvelles variables d'environnement soient appliqu√©es.

    **Utilisation de fichiers**

    La premi√®re chose √† faire est de cr√©er les dossiers :

    ```shell
    mkdir -p ./bw-data/configs/server-http
    ```

    Vous pouvez maintenant √©crire vos configurations :

    ```shell
    echo "location /hello {
      default_type 'text/plain';
      content_by_lua_block {
        ngx.say('world')
      }
    }" > ./bw-data/configs/server-http/hello-world.conf
    ```

    √âtant donn√© que le Scheduler s'ex√©cute en tant qu'utilisateur non privil√©gi√© avec UID et GID 101, vous devrez modifier les autorisations :

    ```shell
    chown -R root:101 bw-data && \
    chmod -R 770 bw-data
    ```

    Au d√©marrage du conteneur de l'ordonnanceur, vous devrez monter le dossier sur /data :

    ```bash
    docker run -d \
        --name bunkerweb-aio \
        -v ./bw-data:/data \
        -p 80:8080/tcp \
        -p 443:8443/tcp \
        -p 443:8443/udp \
        bunkerity/bunkerweb-all-in-one:1.6.6
    ```

=== "Docker"

    Lorsque vous utilisez l'int√©gration [Docker](integrations.md#docker), vous avez deux options pour ajouter des configurations personnalis√©es :

    - Utilisation de param√®tres sp√©cifiques `*_CUSTOM_CONF_*` comme variables d'environnement (recommand√©)
    - √âcriture des fichiers .conf sur le volume mont√© sur /data de l'ordonnanceur

    **Utilisation des param√®tres**

    Les param√®tres √† utiliser doivent suivre le sch√©ma `<SITE>_CUSTOM_CONF_<TYPE>_<NAME>` :

    - `<SITE>` : nom de serveur primaire facultatif si le mode multisite est activ√© et que la configuration doit √™tre appliqu√©e √† un service sp√©cifique
    - `<TYPE>` : le type de configuration, les valeurs accept√©es sont `HTTP`, `DEFAULT_SERVER_HTTP` `SERVER_HTTP` `MODSEC` `MODSEC_CRS` `CRS_PLUGINS_BEFORE`, `CRS_PLUGINS_AFTER`, `STREAM` , et `SERVER_STREAM`
    - `<NAME>` : le nom de config sans le suffixe .conf

    Voici un exemple factice utilisant un fichier docker-compose :

    ```yaml
    ...
    bw-scheduler:
      image: bunkerity/bunkerweb-scheduler:1.6.6
      environment:
        - |
          CUSTOM_CONF_SERVER_HTTP_hello-world=
          location /hello {
            default_type 'text/plain';
            content_by_lua_block {
              ngx.say('world')
            }
          }
      ...
    ```

    **Utilisation de fichiers**

    La premi√®re chose √† faire est de cr√©er les dossiers :

    ```shell
    mkdir -p ./bw-data/configs/server-http
    ```

    Vous pouvez maintenant √©crire vos configurations :

    ```nginx
    echo "location /hello {
      default_type 'text/plain';
      content_by_lua_block {
        ngx.say('world')
      }
    }" > ./bw-data/configs/server-http/hello-world.conf
    ```

    √âtant donn√© que le Scheduler s'ex√©cute en tant qu'utilisateur non privil√©gi√© avec UID et GID 101, vous devrez modifier les autorisations :

    ```shell
    chown -R root:101 bw-data && \
    chmod -R 770 bw-data
    ```

    Au d√©marrage du conteneur de l'ordonnanceur, vous devrez monter le dossier sur /data :

    ```yaml
    bw-scheduler:
      image: bunkerity/bunkerweb-scheduler:1.6.6
      volumes:
        - ./bw-data:/data
      ...
    ```

=== "Docker autoconf"

    Lorsque vous utilisez l'int√©gration [Docker autoconf](integrations.md#docker-autoconf), vous avez deux options pour ajouter des configurations personnalis√©es :

    - Utilisation de param√®tres sp√©cifiques `*_CUSTOM_CONF_*` comme √©tiquettes (le plus simple)
    - √âcriture des fichiers .conf sur le volume mont√© sur /data de l'ordonnanceur

    **Utilisation des √©tiquettes**

    !!! warning "Limitations de l'utilisation des √©tiquettes"
        Lorsque vous utilisez des √©tiquettes avec l'int√©gration Docker autoconf, vous ne pouvez appliquer des configurations personnalis√©es que pour le service web correspondant. L'application de **http**, **default-server-http**, **stream** ou de toute configuration globale (comme **server-http** ou **server-stream** pour tous les services) n'est pas possible : vous devrez monter des fichiers √† cet effet.

    Les √©tiquettes √† utiliser doivent suivre le mod√®le `bunkerweb.CUSTOM_CONF_<TYPE>_<NAME>` :

    - `<TYPE>` : le type de configuration, les valeurs accept√©es sont `SERVER_HTTP`, `MODSEC`, `MODSEC_CRS`, `CRS_PLUGINS_BEFORE` `CRS_PLUGINS_AFTER` et `SERVER_STREAM`
    - `<NAME>` : le nom de config sans le suffixe .conf

    Voici un exemple factice utilisant un fichier docker-compose :

    ```yaml
    myapp:
      image: nginxdemos/nginx-hello
      labels:
        - |
          bunkerweb.CUSTOM_CONF_SERVER_HTTP_hello-world=
          location /hello {
            default_type 'text/plain';
            content_by_lua_block {
                ngx.say('world')
            }
          }
      ...
    ```

    **Utilisation de fichiers**

    La premi√®re chose √† faire est de cr√©er les dossiers :

    ```shell
    mkdir -p ./bw-data/configs/server-http
    ```

    Vous pouvez maintenant √©crire vos configurations :

    ```nginx
    echo "location /hello {
      default_type 'text/plain';
      content_by_lua_block {
        ngx.say('world')
      }
    }" > ./bw-data/configs/server-http/hello-world.conf
    ```

    √âtant donn√© que le Scheduler s'ex√©cute en tant qu'utilisateur non privil√©gi√© avec UID et GID 101, vous devrez modifier les autorisations :

    ```shell
    chown -R root:101 bw-data && \
    chmod -R 770 bw-data
    ```

    Au d√©marrage du conteneur de l'ordonnanceur, vous devrez monter le dossier sur /data :

    ```yaml
    bw-scheduler:
      image: bunkerity/bunkerweb-scheduler:1.6.6
      volumes:
        - ./bw-data:/data
      ...
    ```

=== "Kubernetes"

    Lors de l'utilisation de l'[int√©gration Kubernetes](integrations.md#kubernetes),
    les configurations personnalis√©es sont g√©r√©es √† l'aide de [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/).

    Vous n'avez pas besoin de monter la ConfigMap dans un Pod (par exemple en variable d'environnement ou en volume).
    Le pod autoconf surveille les √©v√©nements ConfigMap et applique automatiquement la configuration d√®s qu'une modification est d√©tect√©e.

    Annotez chaque ConfigMap que le contr√¥leur Ingress doit g√©rer :

    - `bunkerweb.io/CONFIG_TYPE` : obligatoire. Choisissez un type pris en charge (`http`, `server-http`, `default-server-http`, `modsec`,
      `modsec-crs`, `crs-plugins-before`, `crs-plugins-after`, `stream`, `server-stream` ou `settings`).
    - `bunkerweb.io/CONFIG_SITE` : optionnel. Indiquez le nom de serveur principal (tel qu'expos√© via votre `Ingress`)
      pour limiter la configuration √† ce service ; laissez vide pour l'appliquer globalement.

    Voici l'exemple :

    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cfg-bunkerweb-all-server-http
      annotations:
        bunkerweb.io/CONFIG_TYPE: "server-http"
    data:
      myconf: |
      location /hello {
        default_type 'text/plain';
        content_by_lua_block {
          ngx.say('world')
        }
      }
    ```

    !!! info "Fonctionnement de la synchronisation"
        - Le contr√¥leur Ingress surveille en continu les ConfigMap annot√©es.
        - Si la variable d'environnement `NAMESPACES` est d√©finie, seules les ConfigMap de ces espaces de noms sont prises en compte.
        - La cr√©ation ou la mise √† jour d'une ConfigMap g√©r√©e d√©clenche imm√©diatement un rechargement de la configuration.
        - La suppression de la ConfigMap ‚Äì ou de l'annotation `bunkerweb.io/CONFIG_TYPE` ‚Äì supprime la configuration personnalis√©e associ√©e.
        - Si vous d√©finissez `bunkerweb.io/CONFIG_SITE`, le service r√©f√©renc√© doit d√©j√† exister ; sinon, la ConfigMap est ignor√©e jusqu'√† son apparition.

    !!! tip "Custom Extra Config"
        Depuis la version `1.6.0`, vous pouvez ajouter ou remplacer des param√®tres en annotant une ConfigMap avec `bunkerweb.io/CONFIG_TYPE=settings`.
        Le contr√¥leur Ingress d'autoconf lit chaque entr√©e de `data` et l'applique comme une variable d'environnement :

        - Sans `bunkerweb.io/CONFIG_SITE`, toutes les cl√©s sont appliqu√©es globalement.
        - Lorsque `bunkerweb.io/CONFIG_SITE` est d√©fini, le contr√¥leur ajoute automatiquement le pr√©fixe `<nom-de-serveur>_` (chaque `/` est remplac√© par `_`) aux cl√©s qui ne sont pas d√©j√† sp√©cifiques. Ajoutez ce pr√©fixe vous-m√™me si vous devez m√©langer des cl√©s globales et sp√©cifiques dans la m√™me ConfigMap.
        - Les noms ou valeurs invalides sont ignor√©s et un avertissement est enregistr√© dans les journaux du contr√¥leur autoconf.

        Exemple :

        ```yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cfg-bunkerweb-extra-settings
          annotations:
            bunkerweb.io/CONFIG_TYPE: "settings"
        data:
          USE_ANTIBOT: "captcha" # multisite setting that will be applied to all services that do not override it
          USE_REDIS: "yes" # global setting that will be applied globally
          ...
        ```

=== "Swarm"

    !!! warning "Obsol√®te"
        L'int√©gration Swarm est obsol√®te et sera supprim√©e dans une future version. Veuillez envisager d'utiliser l'[int√©gration Kubernetes](integrations.md#kubernetes) √† la place.

        **Plus d'informations sont disponibles dans la [documentation de l'int√©gration Swarm](integrations.md#swarm).**

    Lorsque vous utilisez l'[Swarm integration](integrations.md#swarm), les configurations personnalis√©es sont g√©r√©es √† l'aide des [Docker Configs](https://docs.docker.com/engine/swarm/configs/).

    Pour simplifier, vous n'avez m√™me pas besoin d'attacher le Config √† un service : le service d'autoconf √©coute les √©v√©nements Config et mettra √† jour les configurations personnalis√©es lorsque n√©cessaire.

    Lors de la cr√©ation d'un Config, vous devrez ajouter des labels sp√©ciaux :

    * **bunkerweb.CONFIG_TYPE** : doit √™tre d√©fini sur un type de configuration personnalis√© valide (http, server-http, default-server-http, modsec, modsec-crs, crs-plugins-before, crs-plugins-after, stream, server-stream ou settings)
    * **bunkerweb.CONFIG_SITE** : d√©fini sur un nom de serveur pour appliquer la configuration √† ce serveur sp√©cifique (facultatif, sera appliqu√© globalement s'il n'est pas d√©fini)

    Voici l'exemple :

    ```nginx
    echo "location /hello {
      default_type 'text/plain';
      content_by_lua_block {
        ngx.say('world')
      }
    }" | docker config create -l bunkerweb.CONFIG_TYPE=server-http my-config -
    ```

    Il n'y a pas de m√©canisme de mise √† jour : l'alternative est de supprimer une configuration existante √† l'aide puis de `docker config rm` la recr√©er.

## Ex√©cution de nombreux services en production {#running-many-services-in-production}

### CRS mondial

!!! warning "Plugins CRS"
    Lorsque le SCR est charg√© globalement, les **plug-ins SCR ne sont pas pris en charge**. Si vous avez besoin de les utiliser, vous devrez charger le SCR par service.

Si vous utilisez BunkerWeb en production avec un grand nombre de services, et que vous activez la **fonctionnalit√© ModSecurity globalement** avec des r√®gles CRS, le temps n√©cessaire pour charger les configurations BunkerWeb peut devenir trop long, ce qui peut entra√Æner un d√©lai d'expiration.

La solution de contournement consiste √† charger les r√®gles CRS globalement plut√¥t que par service. Ce comportement n'est pas activ√© par d√©faut pour des raisons de compatibilit√© descendante et parce qu'il pr√©sente un inconv√©nient : si vous activez le chargement des r√®gles CRS globales, **il ne sera plus possible de d√©finir des r√®gles modsec-crs** (ex√©cut√©es avant les r√®gles CRS) par service. Cependant, cette limitation peut √™tre contourn√©e en √©crivant des r√®gles d'exclusion globales `modsec-crs` comme suit :

```
SecRule REQUEST_FILENAME "@rx ^/somewhere$" "nolog,phase:4,allow,id:1010,chain"
SecRule REQUEST_HEADERS:Host "@rx ^app1\.example\.com$" "nolog"
```

Vous pouvez activer le chargement global du SCR en d√©finissant `USE_MODSECURITY_GLOBAL_CRS` la valeur . `yes`

### Ajuster max_allowed_packet pour MariaDB/MySQL

Il semble que la valeur par d√©faut du `max_allowed_packet` param√®tre dans les serveurs de bases de donn√©es MariaDB et MySQL ne soit pas suffisante lors de l'utilisation de BunkerWeb avec un grand nombre de services.

Si vous rencontrez des erreurs comme celle-ci, en particulier sur le Scheduler :

```
[Warning] Aborted connection 5 to db: 'db' user: 'bunkerweb' host: '172.20.0.4' (Got a packet bigger than 'max_allowed_packet' bytes)
```

Vous devrez augmenter le `max_allowed_packet` sur votre serveur de base de donn√©es.

## Persistance des interdictions et des signalements {#persistence-of-bans-and-reports}

Par d√©faut, BunkerWeb stocke les bannissements et les rapports dans un magasin de donn√©es Lua local. Bien que simple et efficace, cette configuration signifie que des donn√©es sont perdues lors du red√©marrage de l'instance. Pour vous assurer que les bannissements et les rapports persistent lors des red√©marrages, vous pouvez configurer BunkerWeb pour utiliser un [ serveur Redis](https://redis.io/) ou [Valkey](https://valkey.io/) distant  .

**Pourquoi utiliser Redis/Valkey ?**

Redis et Valkey sont de puissants magasins de donn√©es en m√©moire couramment utilis√©s comme bases de donn√©es, caches et courtiers de messages. Ils sont hautement √©volutifs et prennent en charge une vari√©t√© de structures de donn√©es, notamment :

- **Cha√Ænes**: paires cl√©-valeur de base.
- **Hachages**: paires champ-valeur au sein d'une seule cl√©.
- **Listes**: collections ordonn√©es de cha√Ænes.
- **Ensembles**: collections non ordonn√©es de cha√Ænes uniques.
- **Ensembles tri√©s**: Collections ordonn√©es avec partitions.

En tirant parti de Redis ou de Valkey, BunkerWeb peut stocker de mani√®re persistante les bannissements, les rapports et les donn√©es de cache, garantissant ainsi la durabilit√© et l'√©volutivit√©.

**Activation de la prise en charge Redis/Valkey**

Pour activer la prise en charge de Redis ou Valkey, configurez les param√®tres suivants dans votre fichier de configuration BunkerWeb :

```conf
# Enable Redis/Valkey support
USE_REDIS=yes

# Redis/Valkey server hostname or IP address
REDIS_HOST=<hostname>

# Redis/Valkey server port number (default: 6379)
REDIS_PORT=6379

# Redis/Valkey database number (default: 0)
REDIS_DATABASE=0
```

- **`USE_REDIS`**: R√©glez sur `yes` pour activer l'int√©gration Redis/Valkey.
- **`REDIS_HOST`**: Sp√©cifiez le nom d'h√¥te ou l'adresse IP du serveur Redis/Valkey.
- **`REDIS_PORT`**: Sp√©cifiez le num√©ro de port pour le serveur Redis/Valkey. La valeur par d√©faut est `6379`.
- **`REDIS_DATABASE`**: Indiquez le num√©ro de base de donn√©es Redis/Valkey √† utiliser. La valeur par d√©faut est `0`.

Si vous avez besoin de param√®tres plus avanc√©s, tels que l'authentification, la prise en charge SSL/TLS ou le mode Sentinel, reportez-vous √† la documentation sur les param√®tres du [plug-in Redis](features.md#redis) pour obtenir des conseils d√©taill√©s.

## Prot√©ger les applications UDP/TCP

!!! example "Fonctionnalit√© exp√©rimentale"

	  This feature is not production-ready. Feel free to test it and report us any bug using [issues](https://github.com/bunkerity/bunkerweb/issues) in the GitHub repository.

BunkerWeb offre la possibilit√© de fonctionner comme un **proxy inverse UDP/TCP g√©n√©rique**, ce qui vous permet de prot√©ger toutes les applications bas√©es sur le r√©seau fonctionnant au moins sur la couche 4 du mod√®le OSI. Au lieu d'utiliser le module HTTP "classique", BunkerWeb exploite le [module de flux](https://nginx.org/en/docs/stream/ngx_stream_core_module.html) de NGINX.

Il est important de noter que **tous les param√®tres et fonctionnalit√©s de s√©curit√© ne sont pas disponibles lors de l'utilisation du module de flux**. Vous trouverez de plus amples informations √† ce sujet dans les sections des [fonctionnalit√©s](features.md) de la documentation.

La configuration d'un proxy inverse de base est assez similaire √† la configuration HTTP, car elle implique l'utilisation des m√™mes param√®tres : `USE_REVERSE_PROXY=yes` et `REVERSE_PROXY_HOST=myapp:9000`. M√™me lorsque BunkerWeb est positionn√© derri√®re un √©quilibreur de charge, les param√®tres restent les m√™mes (le **protocole PROXY** √©tant l'option prise en charge pour des raisons √©videntes).

En plus de cela, les param√®tres sp√©cifiques suivants sont utilis√©s :

- `SERVER_TYPE=stream` : activer  le `stream` mode (UDP/TCP g√©n√©rique) au lieu d' `http` un (qui est la valeur par d√©faut)
- `LISTEN_STREAM_PORT=4242` : le port d'√©coute "simple" (sans SSL/TLS) sur lequel BunkerWeb √©coutera
- `LISTEN_STREAM_PORT_SSL=4343` : le port d'√©coute "ssl/tls" sur lequel BunkerWeb √©coutera
- `USE_UDP=no` : √©couter et transf√©rer les paquets UDP au lieu de TCP

Pour la liste compl√®te des param√®tres concernant `stream` le  mode, veuillez vous r√©f√©rer √† la sections des [fonctionnalit√©s](features.md) de la documentation.

!!! tip "Plusieurs ports d'√©coute"

    Depuis la version `1.6.0`, BunkerWeb prend en charge plusieurs ports d'√©coute pour le mode `stream`. Vous pouvez les sp√©cifier √† l'aide des param√®tres `LISTEN_STREAM_PORT` et `LISTEN_STREAM_PORT_SSL`.

    Voici un exemple :

    ```conf
    ...
    LISTEN_STREAM_PORT=4242
    LISTEN_STREAM_PORT_SSL=4343
    LISTEN_STREAM_PORT_1=4244
    LISTEN_STREAM_PORT_SSL_1=4344
    ...
    ```

=== "Tout-en-un"

    Vous devrez ajouter ces param√®tres aux variables d'environnement lors de l'ex√©cution du conteneur All-in-one. Vous devrez √©galement exposer les ports de stream.

    Cet exemple configure BunkerWeb pour agir comme proxy inverse pour deux applications bas√©es sur le mode stream : `app1.example.com` et `app2.example.com`.

    ```bash
    docker run -d \
        --name bunkerweb-aio \
        -v bw-storage:/data \
        -e SERVICE_UI="no" \
        -e SERVER_NAME="app1.example.com app2.example.com" \
        -e MULTISITE="yes" \
        -e USE_REVERSE_PROXY="yes" \
        -e SERVER_TYPE="stream" \
        -e app1.example.com_REVERSE_PROXY_HOST="myapp1:9000" \
        -e app1.example.com_LISTEN_STREAM_PORT="10000" \
        -e app2.example.com_REVERSE_PROXY_HOST="myapp2:9000" \
        -e app2.example.com_LISTEN_STREAM_PORT="20000" \
        -p 80:8080/tcp \
        -p 443:8443/tcp \
        -p 443:8443/udp \
        -p 10000:10000/tcp \
        -p 20000:20000/tcp \
        bunkerity/bunkerweb-all-in-one:1.6.6
    ```

    Veuillez noter que si votre conteneur existe d√©j√†, vous devrez le supprimer et le recr√©er afin que les nouvelles variables d'environnement soient prises en compte.

    Vos applications (`myapp1`, `myapp2`) doivent s'ex√©cuter dans des conteneurs s√©par√©s (ou √™tre autrement accessibles) et leurs noms d'h√¥te/adresses IP (par ex. `myapp1`, `myapp2` utilis√©s dans `_REVERSE_PROXY_HOST`) doivent √™tre r√©solubles et atteignables depuis le conteneur `bunkerweb-aio`. Cela implique g√©n√©ralement de les connecter √† un r√©seau Docker partag√©.

    !!! note "D√©sactiver le service UI"
        Il est recommand√© de d√©sactiver le service d'interface Web (par exemple en d√©finissant la variable d'environnement `SERVICE_UI=no`) car l'interface Web n'est pas compatible avec `SERVER_TYPE=stream`.

=== "Docker"

    Lors de l'utilisation de l'int√©gration Docker, la mani√®re la plus simple de prot√©ger des applications r√©seau existantes est d'ajouter les services au r√©seau `bw-services` :

    ```yaml
    x-bw-api-env: &bw-api-env
      # We use an anchor to avoid repeating the same settings for all services
      API_WHITELIST_IP: "127.0.0.0/8 10.20.30.0/24"
      # Jeton API optionnel pour les appels API authentifi√©s
      API_TOKEN: ""

    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ports:
          - "80:8080" # Keep it if you want to use Let's Encrypt automation when using http challenge type
          - "10000:10000" # app1
          - "20000:20000" # app2
        labels:
          - "bunkerweb.INSTANCE=yes"
        environment:
          <<: *bw-api-env
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-services

      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          <<: *bw-api-env
          BUNKERWEB_INSTANCES: "bunkerweb" # This setting is mandatory to specify the BunkerWeb instance
          SERVER_NAME: "app1.example.com app2.example.com"
          MULTISITE: "yes"
          USE_REVERSE_PROXY: "yes" # Will be applied to all services
          SERVER_TYPE: "stream" # Will be applied to all services
          app1.example.com_REVERSE_PROXY_HOST: "myapp1:9000"
          app1.example.com_LISTEN_STREAM_PORT: "10000"
          app2.example.com_REVERSE_PROXY_HOST: "myapp2:9000"
          app2.example.com_LISTEN_STREAM_PORT: "20000"
        volumes:
          - bw-storage:/data # This is used to persist the cache and other data like the backups
        restart: "unless-stopped"
        networks:
          - bw-universe

      myapp1:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app1" ]
        networks:
          - bw-services

      myapp2:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app2" ]
        networks:
          - bw-services

    volumes:
      bw-storage:

    networks:
      bw-universe:
        name: bw-universe
        ipam:
          driver: default
          config:
            - subnet: 10.20.30.0/24
      bw-services:
        name: bw-services
    ```

=== "Docker autoconf"

    Avant d'ex√©cuter la pile de l'int√©gration [Docker autoconf](integrations.md#docker-autoconf) sur votre machine, vous devrez modifier les ports :

    ```yaml
    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ports:
          - "80:8080" # Keep it if you want to use Let's Encrypt automation when using http challenge type
          - "10000:10000" # app1
          - "20000:20000" # app2
    ...
    ```

    Une fois la pile en cours d'ex√©cution, vous pouvez connecter vos applications existantes au r√©seau `bw-services` et configurer BunkerWeb avec des `labels` :

    ```yaml
    services:
      myapp1:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app1" ]
        networks:
          - bw-services
        labels:
          - "bunkerweb.SERVER_NAME=app1.example.com"
          - "bunkerweb.SERVER_TYPE=stream"
          - "bunkerweb.USE_REVERSE_PROXY=yes"
          - "bunkerweb.REVERSE_PROXY_HOST=myapp1:9000"
          - "bunkerweb.LISTEN_STREAM_PORT=10000"

      myapp2:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app2" ]
        networks:
          - bw-services
        labels:
          - "bunkerweb.SERVER_NAME=app2.example.com"
          - "bunkerweb.SERVER_TYPE=stream"
          - "bunkerweb.USE_REVERSE_PROXY=yes"
          - "bunkerweb.REVERSE_PROXY_HOST=myapp2:9000"
          - "bunkerweb.LISTEN_STREAM_PORT=20000"

    networks:
      bw-services:
        external: true
        name: bw-services
    ```

=== "Kubernetes"

    !!! example "Fonctionnalit√© exp√©rimentale"

        Actuellement, les [Ingresses](https://kubernetes.io/docs/concepts/services-networking/ingress/) ne prennent pas en charge le mode `stream`. **Ce que nous proposons ici est une solution de contournement pour le faire fonctionner.**

        N'h√©sitez pas √† le tester et √† nous signaler tout bug en ouvrant une issue via [issues](https://github.com/bunkerity/bunkerweb/issues) du d√©p√¥t GitHub.

    Avant d'ex√©cuter la pile de l'[int√©gration Kubernetes](integrations.md#kubernetes) sur votre machine, vous devrez ouvrir les ports sur votre √©quilibreur de charge :

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: lb
    spec:
      type: LoadBalancer
      ports:
        - name: http # Keep it if you want to use Let's Encrypt automation when using http challenge type
          port: 80
          targetPort: 8080
        - name: app1
          port: 10000
          targetPort: 10000
        - name: app2
          port: 20000
          targetPort: 20000
      selector:
        app: bunkerweb
    ```

    Une fois la pile en cours d'ex√©cution, vous pouvez cr√©er vos ressources Ingress :

    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: ingress
      namespace: services
      annotations:
        bunkerweb.io/SERVER_TYPE: "stream" # Will be applied to all services
        bunkerweb.io/app1.example.com_LISTEN_STREAM_PORT: "10000"
        bunkerweb.io/app2.example.com_LISTEN_STREAM_PORT: "20000"
    spec:
      rules:
        - host: app1.example.com
          http:
            paths:
              - path: / # This isn't used in stream mode but is required
                pathType: Prefix
                backend:
                  service:
                    name: svc-app1
                    port:
                      number: 9000
        - host: app2.example.com
          http:
            paths:
              - path: / # This isn't used in stream mode but is required
                pathType: Prefix
                backend:
                  service:
                    name: svc-app2
                    port:
                      number: 9000
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: app1
      namespace: services
      labels:
        app: app1
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: app1
      template:
        metadata:
          labels:
            app: app1
        spec:
          containers:
            - name: app1
              image: istio/tcp-echo-server:1.3
              args: ["9000", "app1"]
              ports:
                - containerPort: 9000
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-app1
      namespace: services
    spec:
      selector:
        app: app1
      ports:
        - protocol: TCP
          port: 9000
          targetPort: 9000
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: app2
      namespace: services
      labels:
        app: app2
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: app2
      template:
        metadata:
          labels:
            app: app2
        spec:
          containers:
            - name: app2
              image: istio/tcp-echo-server:1.3
              args: ["9000", "app2"]
              ports:
                - containerPort: 9000
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: svc-app2
      namespace: services
    spec:
      selector:
        app: app2
      ports:
        - protocol: TCP
          port: 9000
          targetPort: 9000
    ```

=== "Linux"

    Vous devrez ajouter ces param√®tres au fichier /etc/bunkerweb/variables.env :

    ```conf
    ...
    SERVER_NAME=app1.example.com app2.example.com
    MULTISITE=yes
    USE_REVERSE_PROXY=yes
    SERVER_TYPE=stream
    app1.example.com_REVERSE_PROXY_HOST=myapp1.domain.or.ip:9000
    app1.example.com_LISTEN_STREAM_PORT=10000
    app2.example.com_REVERSE_PROXY_HOST=myapp2.domain.or.ip:9000
    app2.example.com_LISTEN_STREAM_PORT=20000
    ...
    ```

    V√©rifions maintenant l'√©tat du Scheduler :

    ```shell
    systemctl status bunkerweb-scheduler
    ```

    S'ils sont d√©j√† en cours d'ex√©cution, nous pouvons le recharger :

    ```shell
    systemctl reload bunkerweb-scheduler
    ```

    Sinon, nous devrons le d√©marrer :

    ```shell
    systemctl start bunkerweb-scheduler
    ```

=== "Swarm"

    !!! warning "Obsol√®te"
        L'int√©gration Swarm est obsol√®te et sera supprim√©e dans une future version. Veuillez envisager d'utiliser l'[int√©gration Kubernetes](integrations.md#kubernetes) √† la place.

        **Plus d'informations sont disponibles dans la [documentation de l'int√©gration Swarm](integrations.md#swarm).**

    Avant d'ex√©cuter la pile de l'int√©gration [Swarm](integrations.md#swarm) sur votre machine, vous devrez modifier les ports :

    ```yaml
    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ports:
          # Keep it if you want to use Let's Encrypt automation when using http challenge type
          - published: 80
            target: 8080
            mode: host
            protocol: tcp
          # app1
          - published: 10000
            target: 10000
            mode: host
            protocol: tcp
          # app2
          - published: 20000
            target: 20000
            mode: host
            protocol: tcp
    ...
    ```

    Une fois la pile en cours d'ex√©cution, vous pouvez connecter vos applications existantes au r√©seau `bw-services` et configurer BunkerWeb √† l'aide d'√©tiquettes :

    ```yaml
    services:

      myapp1:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app1" ]
        networks:
          - bw-services
        deploy:
          placement:
            constraints:
              - "node.role==worker"
          labels:
            - "bunkerweb.SERVER_NAME=app1.example.com"
            - "bunkerweb.SERVER_TYPE=stream"
            - "bunkerweb.USE_REVERSE_PROXY=yes"
            - "bunkerweb.REVERSE_PROXY_HOST=myapp1:9000"
            - "bunkerweb.LISTEN_STREAM_PORT=10000"

      myapp2:
        image: istio/tcp-echo-server:1.3
        command: [ "9000", "app2" ]
        networks:
          - bw-services
        deploy:
          placement:
            constraints:
              - "node.role==worker"
          labels:
            - "bunkerweb.SERVER_NAME=app2.example.com"
            - "bunkerweb.SERVER_TYPE=stream"
            - "bunkerweb.USE_REVERSE_PROXY=yes"
            - "bunkerweb.REVERSE_PROXY_HOST=myapp2:9000"
            - "bunkerweb.LISTEN_STREAM_PORT=20000"

    networks:
      bw-services:
        external: true
        name: bw-services
    ```

## Le PHP

!!! example "Fonctionnalit√© exp√©rimentale"
	  Pour le moment, le support PHP avec BunkerWeb est encore en version b√™ta et nous vous recommandons d'utiliser une architecture de proxy inverse si vous le pouvez. D'ailleurs, PHP n'est pas du tout pris en charge pour certaines int√©grations comme Kubernetes.

BunkerWeb prend en charge PHP en utilisant des  instances [PHP-FPM externes ou ](https://www.php.net/manual/en/install.fpm.php) distantes. Nous supposerons que vous √™tes d√©j√† familiaris√© avec la gestion de ce type de services.

 Les param√®tres suivants peuvent √™tre utilis√©s :

- `REMOTE_PHP` : Nom d'h√¥te de l'instance PHP-FPM distante.
- `REMOTE_PHP_PATH` : Dossier racine contenant les fichiers dans l'instance PHP-FPM distante.
- `REMOTE_PHP_PORT` : Port de l'instance PHP-FPM distante (*9000 par d√©faut*).
- `LOCAL_PHP` : Chemin d'acc√®s au fichier socket local de l'instance PHP-FPM.
- `LOCAL_PHP_PATH` : Dossier racine contenant les fichiers dans l'instance locale PHP-FPM.

=== "Tout-en-un"

    Lorsque vous utilisez l'image [Tout-en-un](integrations.md#all-in-one-aio-image), pour prendre en charge les applications PHP, vous devrez :

    - Montez vos fichiers PHP dans le `/var/www/html` dossier de BunkerWeb.
    - Configurez un conteneur PHP-FPM pour votre application et montez le dossier contenant les fichiers PHP.
    - Utilisez les param√®tres sp√©cifiques `REMOTE_PHP` et `REMOTE_PHP_PATH` comme variables d'environnement lors de l'ex√©cution de BunkerWeb.

    Si vous activez le [mode multisite](features.md#multisite-mode), vous devrez cr√©er des r√©pertoires distincts pour chacune de vos applications. Chaque sous-r√©pertoire doit √™tre nomm√© √† l'aide de la premi√®re valeur de `SERVER_NAME`. Voici un exemple fictif :

    ```
    www
    ‚îú‚îÄ‚îÄ app1.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îî‚îÄ‚îÄ app2.example.com
        ‚îî‚îÄ‚îÄ index.php

    2 directories, 2 files
    ```

    Nous supposerons que vos applications PHP se trouvent dans un dossier nomm√© `www`. Veuillez noter que vous devrez corriger les permissions pour que BunkerWeb (UID/GID 101) puisse au moins lire les fichiers et lister les dossiers et PHP-FPM (UID/GID 33 si vous utilisez l' `php:fpm` image) soit le propri√©taire des fichiers et dossiers :

    ```shell
    chown -R 33:101 ./www && \
    find ./www -type f -exec chmod 0640 {} \; && \
    find ./www -type d -exec chmod 0750 {} \;
    ```

    Vous pouvez maintenant ex√©cuter BunkerWeb, le configurer pour votre application PHP et √©galement ex√©cuter les applications PHP. Vous devrez cr√©er un r√©seau Docker personnalis√© pour permettre √† BunkerWeb de communiquer avec vos conteneurs PHP-FPM.

    ```bash
    # Create a custom network
    docker network create php-network

    # Run PHP-FPM containers
    docker run -d --name myapp1-php --network php-network -v ./www/app1.example.com:/app php:fpm
    docker run -d --name myapp2-php --network php-network -v ./www/app2.example.com:/app php:fpm

    # Run BunkerWeb All-in-one
    docker run -d \
        --name bunkerweb-aio \
        --network php-network \
        -v ./www:/var/www/html \
        -v bw-storage:/data \
        -e SERVER_NAME="app1.example.com app2.example.com" \
        -e MULTISITE="yes" \
        -e REMOTE_PHP_PATH="/app" \
        -e app1.example.com_REMOTE_PHP="myapp1-php" \
        -e app2.example.com_REMOTE_PHP="myapp2-php" \
        -p 80:8080/tcp \
        -p 443:8443/tcp \
        -p 443:8443/udp \
        bunkerity/bunkerweb-all-in-one:1.6.6
    ```

    Veuillez noter que si votre conteneur est d√©j√† cr√©√©, vous devrez le supprimer et le recr√©er pour que les nouvelles variables d'environnement soient appliqu√©es.

=== "Docker"

    Lors de l'utilisation de l'int√©gration [Docker](integrations.md#docker), pour prendre en charge les applications PHP, vous devrez :

    - Montez vos fichiers PHP dans le `/var/www/html` dossier de BunkerWeb
    - Configurez un conteneur PHP-FPM pour votre application et montez le dossier contenant les fichiers PHP
    - Utilisez les param√®tres sp√©cifiques `REMOTE_PHP` et `REMOTE_PHP_PATH` comme variables d'environnement lors du d√©marrage de BunkerWeb

    Si vous activez le [mode multisite](features.md#multisite-mode), vous devrez cr√©er des r√©pertoires distincts pour chacune de vos applications. Chaque sous-r√©pertoire doit √™tre nomm√© √† l'aide de la premi√®re valeur de `SERVER_NAME`. Voici un exemple fictif :

    ```
    www
    ‚îú‚îÄ‚îÄ app1.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îú‚îÄ‚îÄ app2.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îî‚îÄ‚îÄ app3.example.com
        ‚îî‚îÄ‚îÄ index.php

    3 directories, 3 files
    ```

    Nous supposerons que vos applications PHP se trouvent dans un dossier nomm√© `www`. Veuillez noter que vous devrez corriger les permissions pour que BunkerWeb (UID/GID 101) puisse au moins lire les fichiers et lister les dossiers et PHP-FPM (UID/GID 33 si vous utilisez l' `php:fpm` image) soit le propri√©taire des fichiers et dossiers :

    ```shell
    chown -R 33:101 ./www && \
    find ./www -type f -exec chmod 0640 {} \; && \
    find ./www -type d -exec chmod 0750 {} \;
    ```

    Vous pouvez maintenant ex√©cuter BunkerWeb, le configurer pour votre application PHP et √©galement ex√©cuter les applications PHP :

    ```yaml
    x-bw-api-env: &bw-api-env
      # We use an anchor to avoid repeating the same settings for all services
      API_WHITELIST_IP: "127.0.0.0/8 10.20.30.0/24"

    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        ports:
          - "80:8080/tcp"
          - "443:8443/tcp"
          - "443:8443/udp" # QUIC
        environment:
          <<: *bw-api-env
        volumes:
          - ./www:/var/www/html
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-services

      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          <<: *bw-api-env
          BUNKERWEB_INSTANCES: "bunkerweb" # This setting is mandatory to specify the BunkerWeb instance
          SERVER_NAME: "app1.example.com app2.example.com"
          MULTISITE: "yes"
          REMOTE_PHP_PATH: "/app" # Will be applied to all services thanks to the MULTISITE setting
          app1.example.com_REMOTE_PHP: "myapp1"
          app2.example.com_REMOTE_PHP: "myapp2"
          app3.example.com_REMOTE_PHP: "myapp3"
        volumes:
          - bw-storage:/data # This is used to persist the cache and other data like the backups
        restart: "unless-stopped"
        networks:
          - bw-universe

      myapp1:
        image: php:fpm
        volumes:
          - ./www/app1.example.com:/app
        networks:
          - bw-services

      myapp2:
        image: php:fpm
        volumes:
          - ./www/app2.example.com:/app
        networks:
          - bw-services

      myapp3:
        image: php:fpm
        volumes:
          - ./www/app3.example.com:/app
        networks:
          - bw-services

    volumes:
      bw-storage:

    networks:
      bw-universe:
        name: bw-universe
        ipam:
          driver: default
          config:
            - subnet: 10.20.30.0/24
      bw-services:
        name: bw-services
    ```

=== "Docker autoconf"

    !!! info "Mode multisite activ√©"
        L'int√©gration [Docker autoconf](integrations.md#docker-autoconf) implique l'utilisation du mode multisite : prot√©ger une application PHP √©quivaut √† prot√©ger plusieurs.

    Lors de l'utilisation de l'int√©gration [Docker autoconf](integrations.md#docker-autoconf), pour prendre en charge les applications PHP, vous devrez :

    - Montez vos fichiers PHP dans le `/var/www/html` dossier de BunkerWeb
    - Configurez un conteneur PHP-FPM pour vos applications et montez le dossier contenant les applications PHP
    - Utilisez les param√®tres sp√©cifiques `REMOTE_PHP` et `REMOTE_PHP_PATH` comme √©tiquettes pour votre conteneur PHP-FPM

    Comme l'autoconf de Docker implique d'utiliser le [mode multisite](features.md#multisite-mode), vous devrez cr√©er des r√©pertoires distincts pour chacune de vos applications. Chaque sous-r√©pertoire doit √™tre nomm√© √† l'aide de la premi√®re valeur de `SERVER_NAME`. Voici un exemple fictif :

    ```
    www
    ‚îú‚îÄ‚îÄ app1.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îú‚îÄ‚îÄ app2.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îî‚îÄ‚îÄ app3.example.com
        ‚îî‚îÄ‚îÄ index.php

    3 directories, 3 files
    ```

    Une fois les dossiers cr√©√©s, copiez vos fichiers et corrigez les permissions afin que BunkerWeb (UID/GID 101) puisse au moins lire les fichiers et lister les dossiers et PHP-FPM (UID/GID 33 si vous utilisez l' `php:fpm` image) soit le propri√©taire des fichiers et dossiers :

    ```shell
    chown -R 33:101 ./www && \
    find ./www -type f -exec chmod 0640 {} \; && \
    find ./www -type d -exec chmod 0750 {} \;
    ```

    Lorsque vous d√©marrez la pile autoconf de BunkerWeb, montez le `www` dossier dans `/var/www/html` le  conteneur **Scheduler** :

    ```yaml
    x-bw-api-env: &bw-api-env
      # We use an anchor to avoid repeating the same settings for all services
      AUTOCONF_MODE: "yes"
      API_WHITELIST_IP: "127.0.0.0/8 10.20.30.0/24"

    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        labels:
          - "bunkerweb.INSTANCE=yes"
        environment:
          <<: *bw-api-env
        volumes:
          - ./www:/var/www/html
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-services

      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          <<: *bw-api-env
          BUNKERWEB_INSTANCES: "" # We don't need to specify the BunkerWeb instance here as they are automatically detected by the autoconf service
          SERVER_NAME: "" # The server name will be filled with services labels
          MULTISITE: "yes" # Mandatory setting for autoconf
          DATABASE_URI: "mariadb+pymysql://bunkerweb:changeme@bw-db:3306/db" # Remember to set a stronger password for the database
        volumes:
          - bw-storage:/data # This is used to persist the cache and other data like the backups
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-db

      bw-autoconf:
        image: bunkerity/bunkerweb-autoconf:1.6.6
        depends_on:
          - bunkerweb
          - bw-docker
        environment:
          AUTOCONF_MODE: "yes"
          DATABASE_URI: "mariadb+pymysql://bunkerweb:changeme@bw-db:3306/db" # Remember to set a stronger password for the database
          DOCKER_HOST: "tcp://bw-docker:2375" # The Docker socket
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-docker
          - bw-db

      bw-docker:
        image: tecnativa/docker-socket-proxy:nightly
        volumes:
          - /var/run/docker.sock:/var/run/docker.sock:ro
        environment:
          CONTAINERS: "1"
          LOG_LEVEL: "warning"
        networks:
          - bw-docker

      bw-db:
        image: mariadb:11
        # We set the max allowed packet size to avoid issues with large queries
        command: --max-allowed-packet=67108864
        environment:
          MYSQL_RANDOM_ROOT_PASSWORD: "yes"
          MYSQL_DATABASE: "db"
          MYSQL_USER: "bunkerweb"
          MYSQL_PASSWORD: "changeme" # Remember to set a stronger password for the database
        volumes:
          - bw-data:/var/lib/mysql
        networks:
          - bw-docker

    volumes:
      bw-data:
      bw-storage:

    networks:
      bw-universe:
        name: bw-universe
        ipam:
          driver: default
          config:
            - subnet: 10.20.30.0/24
      bw-services:
        name: bw-services
      bw-docker:
        name: bw-docker
    ```

    Vous pouvez maintenant cr√©er vos conteneurs PHP-FPM, monter les bons sous-dossiers et utiliser des libell√©s pour configurer BunkerWeb :

    ```yaml
    services:
      myapp1:
          image: php:fpm
          volumes:
            - ./www/app1.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp1
          labels:
            - "bunkerweb.SERVER_NAME=app1.example.com"
            - "bunkerweb.REMOTE_PHP=myapp1"
            - "bunkerweb.REMOTE_PHP_PATH=/app"

      myapp2:
          image: php:fpm
          volumes:
            - ./www/app2.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp2
          labels:
            - "bunkerweb.SERVER_NAME=app2.example.com"
            - "bunkerweb.REMOTE_PHP=myapp2"
            - "bunkerweb.REMOTE_PHP_PATH=/app"

      myapp3:
          image: php:fpm
          volumes:
            - ./www/app3.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp3
          labels:
            - "bunkerweb.SERVER_NAME=app3.example.com"
            - "bunkerweb.REMOTE_PHP=myapp3"
            - "bunkerweb.REMOTE_PHP_PATH=/app"

    networks:
      bw-services:
        external: true
        name: bw-services
    ```

=== "Kubernetes"

    !!! warning "PHP n'est pas pris en charge pour Kubernetes"
      L'int√©gration Kubernetes permet la configuration via [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) et le contr√¥leur BunkerWeb ne prend actuellement en charge que les applications HTTP.

=== "Linux"

    Nous supposerons que vous avez d√©j√† la pile d'int√©gration [Linux integration](integrations.md#linux) en cours d'ex√©cution sur votre machine.

    Par d√©faut, BunkerWeb recherchera les fichiers web dans le dossier /var/www/html. Vous pouvez l'utiliser pour stocker vos applications PHP. Veuillez noter que vous devrez configurer votre service PHP-FPM pour d√©finir l'utilisateur/groupe des processus en cours et le fichier de socket UNIX utilis√© pour communiquer avec BunkerWeb.

    Tout d'abord, assurez-vous que votre instance PHP-FPM peut acc√©der aux fichiers situ√©s dans /var/www/html et que BunkerWeb peut acc√©der au fichier de socket UNIX afin de communiquer avec PHP-FPM. Il est recommand√© d'utiliser un utilisateur distinct tel que www-data pour le service PHP-FPM et d'autoriser le groupe nginx √† acc√©der au fichier de socket UNIX. Voici la configuration PHP-FPM correspondante :

    ```ini
    ...
    [www]
    user = www-data
    group = www-data
    listen = /run/php/php-fpm.sock
    listen.owner = www-data
    listen.group = nginx
    listen.mode = 0660
    ...
    ```

    N'oubliez pas de red√©marrer votre service PHP-FPM :

    ```shell
    systemctl restart php-fpm
    ```

    Si vous activez le [mode multisite](features.md#multisite-mode), vous devrez cr√©er des r√©pertoires distincts pour chacune de vos applications. Chaque sous-r√©pertoire doit √™tre nomm√© en utilisant la premi√®re valeur de `SERVER_NAME`. Voici un exemple fictif :

    ```
    /var/www/html
    ‚îú‚îÄ‚îÄ app1.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îú‚îÄ‚îÄ app2.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îî‚îÄ‚îÄ app3.example.com
        ‚îî‚îÄ‚îÄ index.php

    3 directories, 3 files
    ```

    Veuillez noter que vous devrez corriger les permissions afin que BunkerWeb (groupe `nginx`) puisse au moins lire les fichiers et lister les dossiers, et que PHP-FPM (utilisateur `www-data`, qui peut varier selon votre syst√®me) soit le propri√©taire des fichiers et dossiers¬†:

    ```shell
    chown -R www-data:nginx /var/www/html && \
    find /var/www/html -type f -exec chmod 0640 {} \; && \
    find /var/www/html -type d -exec chmod 0750 {} \;
    ```

    Vous pouvez maintenant √©diter le fichier `/etc/bunkerweb/variable.env` :

    ```conf
    HTTP_PORT=80
    HTTPS_PORT=443
    DNS_RESOLVERS=9.9.9.9 8.8.8.8 8.8.4.4
    API_LISTEN_IP=127.0.0.1
    MULTISITE=yes
    SERVER_NAME=app1.example.com app2.example.com app3.example.com
    app1.example.com_LOCAL_PHP=/run/php/php-fpm.sock
    app1.example.com_LOCAL_PHP_PATH=/var/www/html/app1.example.com
    app2.example.com_LOCAL_PHP=/run/php/php-fpm.sock
    app2.example.com_LOCAL_PHP_PATH=/var/www/html/app2.example.com
    app3.example.com_LOCAL_PHP=/run/php/php-fpm.sock
    app3.example.com_LOCAL_PHP_PATH=/var/www/html/app3.example.com
    ```

    V√©rifions maintenant l'√©tat du Scheduler :

    ```shell
    systemctl status bunkerweb-scheduler
    ```

    S'il est d√©j√† en cours d'ex√©cution, nous pouvons le recharger :

    ```shell
    systemctl reload bunkerweb-scheduler
    ```

    Sinon, nous devrons le d√©marrer :

    ```shell
    systemctl start bunkerweb-scheduler
    ```

=== "Swarm"

    !!! warning "Obsol√®te"
        L'int√©gration Swarm est obsol√®te et sera supprim√©e dans une future version. Veuillez envisager d'utiliser l'[int√©gration Kubernetes](integrations.md#kubernetes) √† la place.

        **Plus d'informations sont disponibles dans la [documentation de l'int√©gration Swarm](integrations.md#swarm).**

    !!! info "Mode multisite activ√©"
        L'int√©gration [Swarm](integrations.md#docker-autoconf) implique l'utilisation du mode multisite : prot√©ger une application PHP √©quivaut √† prot√©ger plusieurs applications.

    !!! info "Volume partag√©"
        L'utilisation de PHP avec l'int√©gration Docker Swarm n√©cessite un volume partag√© entre toutes les instances BunkerWeb et PHP-FPM, ce qui n'est pas couvert dans cette documentation.

    Lors de l'utilisation de l'int√©gration [Swarm](integrations.md#swarm), pour prendre en charge les applications PHP, vous devrez :

    - Montez vos fichiers PHP dans le `/var/www/html` dossier de BunkerWeb
    - Configurez un conteneur PHP-FPM pour vos applications et montez le dossier contenant les applications PHP
    - Utilisez les param√®tres sp√©cifiques `REMOTE_PHP` et `REMOTE_PHP_PATH` comme √©tiquettes pour votre conteneur PHP-FPM

    √âtant donn√© que l'int√©gration de Swarm implique l'utilisation du [mode multisite](features.md#multisite-mode), vous devrez cr√©er des r√©pertoires distincts pour chacune de vos applications. Chaque sous-r√©pertoire doit √™tre nomm√© √† l'aide de la premi√®re valeur de `SERVER_NAME`. Voici un exemple fictif :

    ```
    www
    ‚îú‚îÄ‚îÄ app1.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îú‚îÄ‚îÄ app2.example.com
    ‚îÇ   ‚îî‚îÄ‚îÄ index.php
    ‚îî‚îÄ‚îÄ app3.example.com
        ‚îî‚îÄ‚îÄ index.php

    3 directories, 3 files
    ```

    √Ä titre d'exemple, nous consid√©rerons que vous avez un dossier partag√© mont√© sur vos n≈ìuds de travail sur le point de `/shared` terminaison.

    Une fois les dossiers cr√©√©s, copiez vos fichiers et corrigez les permissions afin que BunkerWeb (UID/GID 101) puisse au moins lire les fichiers et lister les dossiers et PHP-FPM (UID/GID 33 si vous utilisez l' `php:fpm` image) soit le propri√©taire des fichiers et dossiers :

    ```shell
    chown -R 33:101 /shared/www && \
    find /shared/www -type f -exec chmod 0640 {} \; && \
    find /shared/www -type d -exec chmod 0750 {} \;
    ```

    Lorsque vous d√©marrez la pile BunkerWeb, montez le dossier /shared/www sur /var/www/html dans le conteneur **Scheduler** :

    ```yaml
    services:
      bunkerweb:
        image: bunkerity/bunkerweb:1.6.6
        volumes:
          - /shared/www:/var/www/html
    ...
    ```

    Vous pouvez maintenant cr√©er vos services PHP-FPM, monter les sous-dossiers appropri√©s et utiliser des labels pour configurer BunkerWeb :

    ```yaml
    services:
      myapp1:
          image: php:fpm
          volumes:
            - ./www/app1.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp1
          deploy:
            placement:
              constraints:
                - "node.role==worker"
            labels:
              - "bunkerweb.SERVER_NAME=app1.example.com"
              - "bunkerweb.REMOTE_PHP=myapp1"
              - "bunkerweb.REMOTE_PHP_PATH=/app"

      myapp2:
          image: php:fpm
          volumes:
            - ./www/app2.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp2
          deploy:
            placement:
              constraints:
                - "node.role==worker"
            labels:
              - "bunkerweb.SERVER_NAME=app2.example.com"
              - "bunkerweb.REMOTE_PHP=myapp2"
              - "bunkerweb.REMOTE_PHP_PATH=/app"

      myapp3:
          image: php:fpm
          volumes:
            - ./www/app3.example.com:/app
          networks:
            bw-services:
                aliases:
                  - myapp3
          deploy:
            placement:
              constraints:
                - "node.role==worker"
            labels:
              - "bunkerweb.SERVER_NAME=app3.example.com"
              - "bunkerweb.REMOTE_PHP=myapp3"
              - "bunkerweb.REMOTE_PHP_PATH=/app"

    networks:
      bw-services:
        external: true
        name: bw-services
    ```

## IPv6

!!! example "Fonctionnalit√© exp√©rimentale"

    Cette fonctionnalit√© n'est pas pr√™te pour la production. N'h√©sitez pas √† la tester et √† nous signaler tout bug via les [issues](https://github.com/bunkerity/bunkerweb/issues) du d√©p√¥t GitHub.

Par d√©faut, BunkerWeb n'√©coutera que les adresses IPv4 et n'utilisera pas IPv6 pour les communications r√©seau. Si vous souhaitez activer la prise en charge d'IPv6, vous devez d√©finir `USE_IPV6=yes`. Veuillez noter que la configuration IPv6 de votre r√©seau et de votre environnement n'entre pas dans le champ d'application de cette documentation.

=== "Docker / Autoconf / Swarm"

    Tout d'abord, vous devrez configurer le d√©mon Docker pour activer la prise en charge d'IPv6 pour les conteneurs et utiliser ip6tables si n√©cessaire. Voici une configuration d'exemple pour votre fichier /etc/docker/daemon.json :

    ```json
    {
      "experimental": true,
      "ipv6": true,
      "ip6tables": true,
      "fixed-cidr-v6": "fd00:dead:beef::/48"
    }
    ```

    Vous pouvez maintenant red√©marrer le service Docker pour appliquer les modifications :

    ```shell
    systemctl restart docker
    ```

    Une fois Docker configur√© pour prendre en charge IPv6, vous pouvez ajouter le param√®tre `USE_IPV6` et configurer le r√©seau bw-services pour IPv6 :

    ```yaml
    services:
      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          USE_IPv6: "yes"

    ...

    networks:
      bw-services:
        name: bw-services
        enable_ipv6: true
        ipam:
          config:
            - subnet: fd00:13:37::/48
              gateway: fd00:13:37::1

    ...
    ```

=== "Linux"

    Vous devrez ajouter ces param√®tres au fichier /etc/bunkerweb/variables.env :

    ```conf
    ...
    USE_IPV6=yes
    ...
    ```

    V√©rifions maintenant l'√©tat de BunkerWeb :

    ```shell
    systemctl status bunkerweb
    ```

    S'il est d√©j√† en cours d'ex√©cution, nous pouvons le red√©marrer :

    ```shell
    systemctl restart bunkerweb
    ```

    Sinon, nous devrons le d√©marrer :

    ```shell
    systemctl start bunkerweb
    ```

## Options de configuration de journalisation

BunkerWeb offre une configuration de journalisation flexible, vous permettant d'envoyer les journaux vers plusieurs destinations (comme des fichiers, stdout/stderr ou syslog) simultan√©ment. Cela est particuli√®rement utile pour l'int√©gration avec des collecteurs de journaux externes tout en conservant des journaux locaux pour l'interface Web.

Il y a deux cat√©gories principales de journaux √† configurer :

1. **Journaux de service** : Journaux g√©n√©r√©s par les composants BunkerWeb (Scheduler, UI, Autoconf, etc.). Contr√¥l√©s par service via `LOG_TYPES` (et optionnellement `LOG_FILE_PATH`, `LOG_SYSLOG_ADDRESS`, `LOG_SYSLOG_TAG`).
2. **Journaux d'acc√®s et d'erreur** : Journaux d'acc√®s et d'erreur HTTP g√©n√©r√©s par NGINX. Seuls le service `bunkerweb` les utilise (`ACCESS_LOG` / `ERROR_LOG` / `LOG_LEVEL`).

### Journaux de service

Les journaux de service sont contr√¥l√©s par le param√®tre `LOG_TYPES`, qui peut accepter plusieurs valeurs s√©par√©es par des espaces (par exemple, `LOG_TYPES="stderr syslog"`).

| Valeur   | Description                                                                                                |
| :------- | :--------------------------------------------------------------------------------------------------------- |
| `file`   | √âcrit les journaux dans un fichier. Requis pour le visualiseur de journaux de l'interface Web.             |
| `stderr` | √âcrit les journaux vers l'erreur standard. Standard pour les environnements conteneuris√©s (`docker logs`). |
| `syslog` | Envoie les journaux vers un serveur syslog. N√©cessite que `LOG_SYSLOG_ADDRESS` soit d√©fini.                |

Lors de l'utilisation de `syslog`, vous devriez √©galement configurer :

- `LOG_SYSLOG_ADDRESS` : L'adresse du serveur syslog (par exemple, `udp://bw-syslog:514` ou `/dev/log`).
- `LOG_SYSLOG_TAG` : Une √©tiquette unique pour le service (par exemple, `bw-scheduler`) pour distinguer ses entr√©es.
- `LOG_FILE_PATH` : Chemin pour la sortie fichier lorsque `LOG_TYPES` inclut `file` (par exemple, `/var/log/bunkerweb/scheduler.log`).

### Journaux d'acc√®s et d'erreur

Ce sont des journaux NGINX standard, configur√©s via **le service `bunkerweb` uniquement**. Ils prennent en charge plusieurs destinations en suffixant le nom du param√®tre (par exemple, `ACCESS_LOG`, `ACCESS_LOG_1` et le `LOG_FORMAT` correspondant, `LOG_FORMAT_1` ou `ERROR_LOG`, `ERROR_LOG_1` et leur `LOG_LEVEL` respectif, `LOG_LEVEL_1`).

- `ACCESS_LOG` : Destination pour les journaux d'acc√®s (par d√©faut : `/var/log/bunkerweb/access.log`). Accepte un chemin de fichier, `syslog:server=host[:port][,param=value]`, tampon partag√© `memory:name:size`, ou `off` pour d√©sactiver. Voir la [documentation NGINX access_log](https://nginx.org/en/docs/http/ngx_http_log_module.html#access_log) pour plus de d√©tails.
- `ERROR_LOG` : Destination pour les journaux d'erreur (par d√©faut : `/var/log/bunkerweb/error.log`). Accepte un chemin de fichier, `stderr`, `syslog:server=host[:port][,param=value]`, ou tampon partag√© `memory:size`. Voir la [documentation NGINX error_log](https://nginx.org/en/docs/ngx_core_module.html#error_log) pour plus de d√©tails.
- `LOG_LEVEL` : Niveau de verbosit√© des journaux d'erreur (par d√©faut : `notice`).

Ces param√®tres acceptent des valeurs NGINX standard, y compris des chemins de fichiers, `stderr`, `syslog:server=...` (voir la [documentation NGINX syslog](https://nginx.org/en/docs/syslog.html)), ou des tampons de m√©moire partag√©e. Ils prennent en charge plusieurs destinations via des suffixes num√©rot√©s (voir la [convention des param√®tres multiples](features.md#multiple-settings)). Les autres services (Scheduler, UI, Autoconf, etc.) reposent uniquement sur `LOG_TYPES`/`LOG_FILE_PATH`/`LOG_SYSLOG_*`.

**Exemple avec plusieurs journaux d'acc√®s/erreur (bunkerweb uniquement, suffixes num√©rot√©s) :**

```conf
ACCESS_LOG=/var/log/bunkerweb/access.log
ACCESS_LOG_1=syslog:server=unix:/dev/log,tag=bunkerweb
LOG_FORMAT=$host $remote_addr - $request_id $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent"
LOG_FORMAT_1=$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent
ERROR_LOG=/var/log/bunkerweb/error.log
ERROR_LOG_1=syslog:server=unix:/dev/log,tag=bunkerweb
LOG_LEVEL=notice
LOG_LEVEL_1=error
```

### Valeurs par d√©faut et exemples d'int√©gration

=== "Linux"

    **Comportement par d√©faut** : `LOG_TYPES="file"`. Les journaux sont √©crits dans `/var/log/bunkerweb/*.log`.

    **Exemple** : Conserver les fichiers locaux (pour l'interface Web) et les reproduire √©galement vers le syslog syst√®me.

    ```conf
      # Logs de service (√† d√©finir dans /etc/bunkerweb/variables.env ou les fichiers d'environnement sp√©cifiques aux services)
      LOG_TYPES="file syslog"
      LOG_SYSLOG_ADDRESS=/dev/log
      SCHEDULER_LOG_FILE_PATH=/var/log/bunkerweb/scheduler.log
      UI_LOG_FILE_PATH=/var/log/bunkerweb/ui.log
      # ...
      # LOG_SYSLOG_TAG est d√©fini automatiquement par service (remplacez-le par service si n√©cessaire)

      # Logs NGINX (service bunkerweb uniquement ; √† d√©finir dans /etc/bunkerweb/variables.env)
      ACCESS_LOG_1=syslog:server=unix:/dev/log,tag=bunkerweb_access
      ERROR_LOG_1=syslog:server=unix:/dev/log,tag=bunkerweb
    ```

=== "Docker / Autoconf / Swarm"

    **Comportement par d√©faut** : `LOG_TYPES="stderr"`. Les journaux sont visibles via `docker logs`.

    **Exemple (Adapt√© du guide de d√©marrage rapide)** : Conserver `docker logs` (stderr) ET envoyer vers un conteneur syslog central (n√©cessaire pour l'interface Web et CrowdSec).

    ```yaml
    x-bw-env:
      &bw-env # On utilise une ancre pour √©viter de r√©p√©ter les m√™mes param√®tres pour les deux services
      API_WHITELIST_IP: "127.0.0.0/8 10.20.30.0/24" # Assurez-vous de d√©finir la plage IP correcte pour que le Scheduler puisse envoyer la configuration √† l'instance
      # Optionnel : d√©finissez un token API et r√©pliquez-le dans les deux conteneurs
      API_TOKEN: ""
      DATABASE_URI: "mariadb+pymysql://bunkerweb:changeme@bw-db:3306/db" # N'oubliez pas de d√©finir un mot de passe plus fort pour la base de donn√©es
      # Logs des services
      LOG_TYPES: "stderr syslog"
      LOG_SYSLOG_ADDRESS: "udp://bw-syslog:514"
      # LOG_SYSLOG_TAG sera d√©fini automatiquement par service (remplacez-le par service si n√©cessaire)
      # Logs NGINX : envoyer au syslog (bunkerweb uniquement)
      ACCESS_LOG_1: "syslog:server=bw-syslog:514,tag=bunkerweb_access"
      ERROR_LOG_1: "syslog:server=bw-syslog:514,tag=bunkerweb"

    services:
      bunkerweb:
        # Ceci est le nom qui sera utilis√© pour identifier l'instance dans le Scheduler
        image: bunkerity/bunkerweb:1.6.6
        ports:
          - "80:8080/tcp"
          - "443:8443/tcp"
          - "443:8443/udp" # Pour la prise en charge de QUIC / HTTP3
        environment:
          <<: *bw-env # Nous utilisons l'ancre pour √©viter de r√©p√©ter les m√™mes param√®tres pour tous les services
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-services

      bw-scheduler:
        image: bunkerity/bunkerweb-scheduler:1.6.6
        environment:
          <<: *bw-env
          BUNKERWEB_INSTANCES: "bunkerweb" # Assurez-vous de d√©finir le nom d'instance correct
          SERVER_NAME: ""
          MULTISITE: "yes"
          UI_HOST: "http://bw-ui:7000" # Modifiez si n√©cessaire
          USE_REDIS: "yes"
          REDIS_HOST: "redis"
        volumes:
          - bw-storage:/data # Utilis√© pour persister le cache et d'autres donn√©es (sauvegardes, etc.)
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-db

      bw-ui:
        image: bunkerity/bunkerweb-ui:1.6.6
        environment:
          <<: *bw-env
        volumes:
          - bw-logs:/var/log/bunkerweb # Permet √† l'UI de lire les logs syslog
        restart: "unless-stopped"
        networks:
          - bw-universe
          - bw-db

      bw-db:
        image: mariadb:11
        # Nous d√©finissons max_allowed_packet pour √©viter les probl√®mes avec de grandes requ√™tes
        command: --max-allowed-packet=67108864
        environment:
          MYSQL_RANDOM_ROOT_PASSWORD: "yes"
          MYSQL_DATABASE: "db"
          MYSQL_USER: "bunkerweb"
          MYSQL_PASSWORD: "changeme" # N'oubliez pas de d√©finir un mot de passe plus fort pour la base de donn√©es
        volumes:
          - bw-data:/var/lib/mysql
        restart: "unless-stopped"
        networks:
          - bw-db

      redis: # Service Redis pour la persistance des rapports/bans/statistiques
        image: redis:7-alpine
        command: >
          redis-server
          --maxmemory 256mb
          --maxmemory-policy allkeys-lru
          --save 60 1000
          --appendonly yes
        volumes:
          - redis-data:/data
        restart: "unless-stopped"
        networks:
          - bw-universe

      bw-syslog:
        image: balabit/syslog-ng:4.10.2
        cap_add:
          - NET_BIND_SERVICE # Lier aux ports bas
          - NET_BROADCAST # Envoyer des broadcasts
          - NET_RAW # Utiliser des sockets bruts
          - DAC_READ_SEARCH # Lire des fichiers en contournant les permissions
          - DAC_OVERRIDE # Outrepasser les permissions de fichiers
          - CHOWN # Changer les propri√©taires
          - SYSLOG # √âcrire dans les journaux syst√®me
        volumes:
          - bw-logs:/var/log/bunkerweb # Volume utilis√© pour stocker les logs
          - ./syslog-ng.conf:/etc/syslog-ng/syslog-ng.conf # Fichier de configuration syslog-ng
        restart: "unless-stopped"
        networks:
          - bw-universe

    volumes:
      bw-data:
      bw-storage:
      redis-data:
      bw-logs:

    networks:
      bw-universe:
        name: bw-universe
      ipam:
        driver: default
        config:
          - subnet: 10.20.30.0/24 # Assurez-vous de d√©finir la plage IP correcte pour que le Scheduler puisse envoyer la configuration √† l'instance
      bw-services:
        name: bw-services
      bw-db:
        name: bw-db
    ```

### Configuration de syslog-ng

Voici un exemple de fichier `syslog-ng.conf` que vous pouvez utiliser pour rediriger les journaux vers un fichier :

```conf
@version: 4.10

# Configuration de la source pour recevoir les journaux envoy√©s par les services BunkerWeb (ACCESS_LOG / ERROR_LOG et LOG_TYPES=syslog)
source s_net {
  udp(
    ip("0.0.0.0")
  );
};

# Mod√®le pour formater les messages de journalisation
template t_imp {
  template("$MSG\n");
  template_escape(no);
};

# Destination : √©crire les journaux dans des fichiers nomm√©s dynamiquement
destination d_dyna_file {
  file(
    "/var/log/bunkerweb/${PROGRAM}.log"
    template(t_imp)
    owner("101")
    group("101")
    dir_owner("root")
    dir_group("101")
    perm(0440)
    dir_perm(0770)
    create_dirs(yes)
    logrotate(
      enable(yes),
      size(100MB),
      rotations(7)
    )
  );
};

# Chemin de journalisation pour diriger les logs vers des fichiers nomm√©s dynamiquement
log {
  source(s_net);
  destination(d_dyna_file);
};
```

## Meilleures pratiques de journalisation Docker

Lors de l'utilisation de Docker, il est important de g√©rer les journaux des conteneurs pour √©viter qu'ils ne consomment un espace disque excessif. Par d√©faut, Docker utilise le pilote de journalisation `json-file`, ce qui peut entra√Æner des fichiers journaux tr√®s volumineux s'il n'est pas configur√©.

Pour √©viter cela, vous pouvez configurer la rotation des journaux. Cela peut √™tre fait pour des services sp√©cifiques dans votre fichier `docker-compose.yml`, ou globalement pour le d√©mon Docker.

**Configuration par service**

Vous pouvez configurer le pilote de journalisation pour vos services dans votre fichier `docker-compose.yml` afin de faire pivoter automatiquement les journaux. Voici un exemple qui conserve jusqu'√† 10 fichiers journaux de 20 Mo chacun :

```yaml
services:
  bunkerweb:
    image: bunkerity/bunkerweb:1.6.6
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "10"
    ...
```

Cette configuration garantit la rotation des journaux, les emp√™chant de remplir votre disque. Vous pouvez l'appliquer √† n'importe quel service de votre configuration Docker Compose.

**Configuration globale (daemon.json)**

Si vous souhaitez appliquer ces param√®tres de journalisation √† tous les conteneurs de l'h√¥te par d√©faut, vous pouvez configurer le d√©mon Docker en modifiant (ou en cr√©ant) le fichier `/etc/docker/daemon.json` :

```json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "20m",
    "max-file": "10"
  }
}
```

Apr√®s avoir modifi√© `daemon.json`, vous devez red√©marrer le d√©mon Docker pour que les modifications prennent effet :

```shell
sudo systemctl restart docker
```

Cette configuration globale sera h√©rit√©e par tous les conteneurs. Cependant, toute configuration de journalisation d√©finie par service dans un fichier `docker-compose.yml` remplacera les param√®tres globaux dans `daemon.json`.

## R√©glage de la s√©curit√© {#security-tuning}

BunkerWeb offre de nombreuses fonctionnalit√©s de s√©curit√© que vous pouvez configurer avec les [fonctionnalit√©s](features.md). M√™me si les valeurs par d√©faut des param√®tres assurent une "s√©curit√© par d√©faut" minimale, nous vous recommandons vivement de les r√©gler. Ce faisant, vous serez en mesure de vous assurer du niveau de s√©curit√© de votre choix, mais aussi de g√©rer les faux positifs.

!!! tip "Autres fonctionnalit√©s"
    Cette section se concentre uniquement sur le r√©glage de la s√©curit√©, voir la section [fonctionnalit√©s](features.md) de la documentation pour d'autres param√®tres.

<figure markdown>
  ![Vue d'ensemble](assets/img/core-order.svg){ align=center }
  <figcaption>Vue d'ensemble et ordre des plugins de s√©curit√© de base</figcaption>
</figure>

## Int√©gration de la console CrowdSec

Si vous n'√™tes pas d√©j√† familier avec l'int√©gration de la console CrowdSec, [CrowdSec](https://www.crowdsec.net/?utm_campaign=bunkerweb&utm_source=doc) exploite l'intelligence participative pour lutter contre les cybermenaces. Consid√©rez-le comme le "Waze de la cybers√©curit√©" : lorsqu'un serveur est attaqu√©, les autres syst√®mes du monde entier sont alert√©s et prot√©g√©s contre les m√™mes attaquants. Vous pouvez en savoir plus √† ce sujet [ici](https://www.crowdsec.net/about?utm_campaign=bunkerweb&utm_source=blog).

**F√©licitations, votre instance BunkerWeb est maintenant inscrite dans votre console CrowdSec !**

Conseil professionnel : Lorsque vous consultez vos alertes, cliquez sur l'option "colonnes" et cochez la case "contexte" pour acc√©der aux donn√©es sp√©cifiques √† BunkerWeb.

<figure markdown>
  ![Vue d'ensemble](assets/img/crowdity4.png){ align=center }
  <figcaption>Donn√©es BunkerWeb affich√©es dans la colonne de contexte</figcaption>
</figure>

## Surveillance et rapports

### Monitoring <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge de STREAM :x:

Le plugin de surveillance vous permet de collecter et de r√©cup√©rer des m√©triques sur BunkerWeb. En l'activant, votre ou vos instances commenceront √† collecter diverses donn√©es li√©es aux attaques, aux requ√™tes et aux performances. Vous pouvez ensuite les r√©cup√©rer en appelant r√©guli√®rement le point de terminaison de l' `/monitoring` API ou en utilisant d'autres plugins comme celui de l'exportateur Prometheus.

**Liste des fonctionnalit√©s**

- Permettre la collecte de diverses m√©triques BunkerWeb
- R√©cup√©rer des m√©triques √† partir de l'API
- Utilisation en combinaison avec d'autres plugins (par exemple Prometheus exporter)
- D√©di√©e √† la page d'interface utilisateur pour surveiller vos instances

**Liste des param√®tres**

| R√©glage                        | D√©faut | Contexte | Multiple | Description                                                        |
| ------------------------------ | ------ | -------- | -------- | ------------------------------------------------------------------ |
| `USE_MONITORING`               | `yes`  | global   | Non      | Activez la surveillance de BunkerWeb.                              |
| `MONITORING_METRICS_DICT_SIZE` | `10M`  | global   | Non      | Taille du dictionnaire pour stocker les m√©triques de surveillance. |

### Prometheus exporter <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge de STREAM :x:

Le plugin d'exportation Prometheus ajoute un [exportateur Prometheus](https://prometheus.io/docs/instrumenting/exporters/) sur votre ou vos instances BunkerWeb. Lorsqu'elle est activ√©e, vous pouvez configurer votre ou vos instances Prometheus pour r√©cup√©rer un point de terminaison sp√©cifique sur Bunkerweb et collecter des m√©triques internes.

Nous fournissons √©galement un [tableau de bord Grafana](https://grafana.com/grafana/dashboards/20755-bunkerweb/) que vous pouvez importer dans votre propre instance et connecter √† votre propre source de donn√©es Prometheus.

**Veuillez noter que l'utilisation du plugin d'exportation Prometheus n√©cessite d'activer le plugin de surveillance (`USE_MONITORING=yes`)**

**Liste des fonctionnalit√©s**

- L'exportateur Prometheus fournit des m√©triques internes √† BunkerWeb
- Port d√©di√© et configurable, IP et URL d'√©coute
- Liste blanche IP/r√©seau pour une s√©curit√© maximale

**Liste des param√®tres**

| R√©glage                        | Dd√©faut                                               | Contexte | Multiple | Description                                                                                              |
| ------------------------------ | ----------------------------------------------------- | -------- | -------- | -------------------------------------------------------------------------------------------------------- |
| `USE_PROMETHEUS_EXPORTER`      | `no`                                                  | global   | Non      | Activez l'exportation Prometheus.                                                                        |
| `PROMETHEUS_EXPORTER_IP`       | `0.0.0.0`                                             | global   | Non      | IP d'√©coute de l'exportateur Prometheus.                                                                 |
| `PROMETHEUS_EXPORTER_PORT`     | `9113`                                                | global   | Non      | Port d'√©coute de l'exportateur Prometheus.                                                               |
| `PROMETHEUS_EXPORTER_URL`      | `/metrics`                                            | global   | Non      | URL HTTP de l'exportateur Prometheus.                                                                    |
| `PROMETHEUS_EXPORTER_ALLOW_IP` | `127.0.0.0/8 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16` | global   | Non      | Liste des adresses IP/r√©seaux autoris√©s √† contacter le point de terminaison de l'exportateur Prometheus. |

### Reporting <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge de STREAM :x:

!!! warning "Plugin de surveillance n√©cessaire"
    Ce plug-in n√©cessite l'installation et l'activation du plug-in Monitoring Pro avec le `USE_MONITORING` param√®tre d√©fini sur `yes`.

Le plugin Reporting fournit une solution compl√®te pour la communication r√©guli√®re de donn√©es importantes de BunkerWeb, y compris les statistiques mondiales, les attaques, les bannissements, les demandes, les raisons et les informations AS. Il offre un large √©ventail de fonctionnalit√©s, notamment la cr√©ation automatique de rapports, des options de personnalisation et une int√©gration transparente avec le plugin monitoring pro. Avec le plugin Reporting, vous pouvez facilement g√©n√©rer et g√©rer des rapports pour surveiller les performances et la s√©curit√© de votre application.

**Liste des fonctionnalit√©s**

- Rapports r√©guliers sur les donn√©es importantes de BunkerWeb, y compris les statistiques mondiales, les attaques, les bannissements, les demandes, les raisons et les informations sur les SA.
- Int√©gration avec le plug-in Monitoring Pro pour une int√©gration transparente et des capacit√©s de reporting am√©lior√©es.
- Prise en charge des webhooks (classique, Discord et Slack) pour les notifications en temps r√©el.
- Prise en charge de SMTP pour les notifications par e-mail.
- Options de configuration pour plus de personnalisation et de flexibilit√©.

**Liste des param√®tres**

| R√©glage                        | Par d√©faut         | Contexte | Description                                                                                |
| ------------------------------ | ------------------ | -------- | ------------------------------------------------------------------------------------------ |
| `USE_REPORTING_SMTP`           | `no`               | global   | Activer l'envoi du rapport par e-mail (HTML).                                              |
| `USE_REPORTING_WEBHOOK`        | `no`               | global   | Activer l'envoi du rapport via webhook (Markdown).                                         |
| `REPORTING_SCHEDULE`           | `weekly`           | global   | Cadence du rapport : `daily`, `weekly` ou `monthly`.                                       |
| `REPORTING_WEBHOOK_URLS`       |                    | global   | URLs de webhook s√©par√©es par des espaces ; Discord et Slack sont d√©tect√©s automatiquement. |
| `REPORTING_SMTP_EMAILS`        |                    | global   | Destinataires e-mail s√©par√©s par des espaces.                                              |
| `REPORTING_SMTP_HOST`          |                    | global   | Nom d'h√¥te ou IP du serveur SMTP.                                                          |
| `REPORTING_SMTP_PORT`          | `465`              | global   | Port SMTP. Utilisez `465` pour SSL, `587` pour TLS.                                        |
| `REPORTING_SMTP_FROM_EMAIL`    |                    | global   | Adresse de l'exp√©diteur (d√©sactivez la 2FA si n√©cessaire).                                 |
| `REPORTING_SMTP_FROM_USER`     |                    | global   | Nom d'utilisateur SMTP (utilise l'adresse d'envoi si seul le mot de passe est fourni).     |
| `REPORTING_SMTP_FROM_PASSWORD` |                    | global   | Mot de passe SMTP.                                                                         |
| `REPORTING_SMTP_SSL`           | `SSL`              | global   | S√©curit√© de connexion : `no`, `SSL` ou `TLS`.                                              |
| `REPORTING_SMTP_SUBJECT`       | `BunkerWeb Report` | global   | Objet des envois e-mail.                                                                   |

!!! info "Information et comportement"
    - `REPORTING_SMTP_EMAILS` est requis quand l'envoi SMTP est activ√© ; `REPORTING_WEBHOOK_URLS` est requis quand les webhooks sont activ√©s.
    - Si les webhooks et SMTP √©chouent tous, une nouvelle tentative est effectu√©e lors de la prochaine ex√©cution planifi√©e.
    - Les mod√®les HTML et Markdown se trouvent dans `reporting/files/` ; personnalisez-les prudemment pour conserver les variables.

## Sauvegarde et restauration

### Backup S3 <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge STREAM :white_check_mark:

L'outil Backup S3 automatise de mani√®re transparente la protection des donn√©es, √† l'instar du plug-in de sauvegarde communautaire. Cependant, il se distingue par le stockage s√©curis√© des sauvegardes directement dans un compartiment S3.

En activant cette fonctionnalit√©, vous prot√©gez de mani√®re proactive **l'int√©grit√© de vos donn√©es**. Le stockage √† **distance** des sauvegardes prot√®ge les informations cruciales contre les menaces telles que ** les pannes mat√©rielles**, **les cyberattaques** ou **les catastrophes naturelles**. Cela garantit √† la fois **la s√©curit√©** et **la disponibilit√©**, ce qui permet une r√©cup√©ration rapide en cas ** d'√©v√©nements inattendus**, pr√©servant la **continuit√© op√©rationnelle** et garantissant **la tranquillit√© d'esprit**.

??? warning "Informations pour les utilisateurs de Red Hat Enterprise Linux (RHEL) 8.9"
    Si vous utilisez **RHEL 8.9** et que vous pr√©voyez d'utiliser une **base de donn√©es externe**, vous devez installer le `mysql-community-client` package pour vous assurer que la `mysqldump` commande est disponible. Vous pouvez installer le package en ex√©cutant les commandes suivantes :

    === "MySQL/MariaDB"

        1. **Installez le paquet de configuration du d√©p√¥t MySQL**

          ```bash
          sudo dnf install https://dev.mysql.com/get/mysql80-community-release-el8-9.noarch.rpm
          ```

        2. **Activez le d√©p√¥t MySQL**

          ```bash
          sudo dnf config-manager --enable mysql80-community
          ```

        3. **Installez le client MySQL**

          ```bash
          sudo dnf install mysql-community-client
          ```

    === "PostgreSQL"

        1. **Installez le paquet de configuration du d√©p√¥t PostgreSQL**

          ```bash
          dnf install "https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-$(uname -m)/pgdg-redhat-repo-latest.noarch.rpm"
          ```

        2. **Installez le client PostgreSQL**

          ```bash
          dnf install postgresql<version>
          ```

**Liste des fonctionnalit√©s**

- Sauvegarde automatique des donn√©es dans un compartiment S3
- Options de planification flexibles : quotidienne, hebdomadaire ou mensuelle
- Gestion de la rotation pour contr√¥ler le nombre de sauvegardes √† conserver
- Niveau de compression personnalisable pour les fichiers de sauvegarde

**Liste des param√®tres**

| R√©glage                       | Faire d√©faut | Contexte | Description                                           |
| ----------------------------- | ------------ | -------- | ----------------------------------------------------- |
| `USE_BACKUP_S3`               | `no`         | global   | Activer ou d√©sactiver la fonction de sauvegarde S3    |
| `BACKUP_S3_SCHEDULE`          | `daily`      | global   | La fr√©quence de la sauvegarde                         |
| `BACKUP_S3_ROTATION`          | `7`          | global   | Le nombre de sauvegardes √† conserver                  |
| `BACKUP_S3_ENDPOINT`          |              | global   | Le point de terminaison S3                            |
| `BACKUP_S3_BUCKET`            |              | global   | Le godet S3                                           |
| `BACKUP_S3_DIR`               |              | global   | L'annuaire S3                                         |
| `BACKUP_S3_REGION`            |              | global   | La r√©gion S3                                          |
| `BACKUP_S3_ACCESS_KEY_ID`     |              | global   | L'ID de la cl√© d'acc√®s S3                             |
| `BACKUP_S3_ACCESS_KEY_SECRET` |              | global   | Le secret de la cl√© d'acc√®s S3                        |
| `BACKUP_S3_COMP_LEVEL`        | `6`          | global   | Le niveau de compression du fichier zip de sauvegarde |

#### Sauvegarde manuelle

Pour lancer manuellement une sauvegarde, ex√©cutez la commande suivante :

=== "Linux"

    ```bash
    bwcli plugin backup_s3 save
    ```

=== "Docker"

    ```bash
    docker exec -it <scheduler_container> bwcli plugin backup_s3 save
    ```

Cette commande cr√©e une sauvegarde de votre base de donn√©es et la stocke dans le compartiment S3 sp√©cifi√© dans le `BACKUP_S3_BUCKET` param√®tre.

Vous pouvez √©galement sp√©cifier un compartiment S3 personnalis√© pour la sauvegarde en fournissant la variable d' `BACKUP_S3_BUCKET` environnement lors de l'ex√©cution de la commande :

=== "Linux"

    ```bash
    BACKUP_S3_BUCKET=your-bucket-name bwcli plugin backup_s3 save
    ```

=== "Docker"

    ```bash
    docker exec -it -e BACKUP_S3_BUCKET=your-bucket-name <scheduler_container> bwcli plugin backup_s3 save
    ```

!!! note "Sp√©cifications pour MariaDB/MySQL"

    Si vous utilisez MariaDB/MySQL, vous pouvez rencontrer l'erreur suivante lors de la sauvegarde de votre base de donn√©es :

    ```bash
    caching_sha2_password could not be loaded: Error loading shared library /usr/lib/mariadb/plugin/caching_sha2_password.so
    ```

    Pour r√©soudre ce probl√®me, vous pouvez ex√©cuter la commande suivante pour changer le plugin d'authentification en `mysql_native_password` :

    ```sql
    ALTER USER 'yourusername'@'localhost' IDENTIFIED WITH mysql_native_password BY 'youpassword';
    ```

    Si vous utilisez l'int√©gration Docker, vous pouvez ajouter la commande suivante au fichier `docker-compose.yml` pour changer automatiquement le plugin d'authentification :

    === "MariaDB"

        ```yaml
        bw-db:
            image: mariadb:<version>
            command: --default-authentication-plugin=mysql_native_password
            ...
        ```

    === "MySQL"

        ```yaml
        bw-db:
            image: mysql:<version>
            command: --default-authentication-plugin=mysql_native_password
            ...
        ```

#### Restauration manuelle

Pour lancer manuellement une restauration, ex√©cutez la commande suivante :

=== "Linux"

    ```bash
    bwcli plugin backup_s3 restore
    ```

=== "Docker"

    ```bash
    docker exec -it <scheduler_container> bwcli plugin backup_s3 restore
    ```

Cette commande cr√©e une sauvegarde temporaire de votre base de donn√©es dans le compartiment S3 sp√©cifi√© dans le `BACKUP_S3_BUCKET` param√®tre et restaure votre base de donn√©es √† la derni√®re sauvegarde disponible dans le compartiment.

Vous pouvez √©galement sp√©cifier un fichier de sauvegarde personnalis√© pour la restauration en fournissant le chemin d'acc√®s √† celui-ci en tant qu'argument lors de l'ex√©cution de la commande :

=== "Linux"

    ```bash
    bwcli plugin backup_s3 restore s3_backup_file.zip
    ```

=== "Docker"

    ```bash
    docker exec -it <scheduler_container> bwcli plugin backup restore s3_backup_file.zip
    ```

!!! example "En cas de panne"

    Don't worry if the restore fails, you can always restore your database to the previous state by executing the command again as a backup is created before the restore:

    === "Linux"

        ```bash
        bwcli plugin backup_s3 restore
        ```

    === "Docker"

        ```bash
        docker exec -it <scheduler_container> bwcli plugin backup_s3 restore
        ```

## Migration <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge STREAM :white_check_mark:

Le plug-in de migration **r√©volutionne les transferts de** configuration BunkerWeb entre les instances gr√¢ce √† son **interface Web conviviale**, simplifiant ainsi l'ensemble du parcours de migration. Que vous mettiez √† niveau des syst√®mes, que vous fassiez √©voluer une infrastructure ou que vous transformiez d'environnement, cet outil vous permet de transf√©rer sans effort les **param√®tres, les pr√©f√©rences et les donn√©es** avec une facilit√© et une confiance in√©gal√©es. Dites adieu aux processus manuels fastidieux et bonjour √† une exp√©rience de **migration transparente et sans tracas**.

**Liste des fonctionnalit√©s**

- **Migration sans effort :** Transf√©rez facilement les configurations BunkerWeb entre les instances sans les complexit√©s des proc√©dures manuelles.

- **Interface Web intuitive :** Naviguez sans effort dans le processus de migration gr√¢ce √† une interface Web conviviale con√ßue pour un fonctionnement intuitif.

- **Compatibilit√© entre bases de donn√©es :** profitez d'une migration transparente sur diverses plates-formes de bases de donn√©es, notamment SQLite, MySQL, MariaDB et PostgreSQL, garantissant la compatibilit√© avec votre environnement de base de donn√©es pr√©f√©r√©.

### Cr√©er un fichier de migration

Pour cr√©er manuellement un fichier de migration, ex√©cutez la commande suivante :

=== "Linux"

    ```bash
    bwcli plugin migration create /path/to/migration/file
    ```

=== "Docker"

    1. Cr√©ez un fichier de migration :

        ```bash
        docker exec -it <scheduler_container> bwcli plugin migration create /path/to/migration/file
        ```

    2. Copiez le fichier de migration sur votre ordinateur local :

        ```bash
        docker cp <scheduler_container>:/path/to/migration/file /path/to/migration/file
        ```

Cette commande cr√©era une sauvegarde de votre base de donn√©es et la stockera dans le r√©pertoire de sauvegarde sp√©cifi√© dans la commande.

!!! note "Sp√©cifications pour MariaDB/MySQL"

    Si vous utilisez MariaDB/MySQL, vous pouvez rencontrer l'erreur suivante lors de la sauvegarde de votre base de donn√©es :

    ```bash
    caching_sha2_password could not be loaded: Error loading shared library /usr/lib/mariadb/plugin/caching_sha2_password.so
    ```

    Pour r√©soudre ce probl√®me, vous pouvez ex√©cuter la commande suivante pour changer le plugin d'authentification en `mysql_native_password` :

    ```sql
    ALTER USER 'yourusername'@'localhost' IDENTIFIED WITH mysql_native_password BY 'youpassword';
    ```

    Si vous utilisez l'int√©gration Docker, vous pouvez ajouter la commande suivante au fichier `docker-compose.yml` pour changer automatiquement le plugin d'authentification :

    === "MariaDB"

        ```yaml
        bw-db:
            image: mariadb:<version>
            command: --default-authentication-plugin=mysql_native_password
            ...
        ```

    === "MySQL"

        ```yaml
        bw-db:
            image: mysql:<version>
            command: --default-authentication-plugin=mysql_native_password
            ...
        ```

### Initialiser une migration

Pour initialiser manuellement une migration, ex√©cutez la commande suivante :

=== "Linux"

    ```bash
    bwcli plugin migration migrate /path/to/migration/file
    ```

=== "Docker"

    1. Copiez le fichier de migration dans le conteneur :

        ```bash
        docker cp /path/to/migration/file <scheduler_container>:/path/to/migration/file
        ```

    2. Initialisez la migration :

        ```bash
        docker exec -it <scheduler_container> bwcli plugin migration migrate /path/to/migration/file
        ```

=== "Tout-en-un"

    1. Copiez le fichier de migration dans le conteneur :

        ```bash
        docker cp /path/to/migration/file bunkerweb-aio:/path/to/migration/file
        ```

    2. Initialisez la migration :

        ```bash
        docker exec -it bunkerweb-aio bwcli plugin migration migrate /path/to/migration/file
        ```

Cette commande migre de mani√®re transparente vos donn√©es BunkerWeb pour qu'elles correspondent pr√©cis√©ment √† la configuration d√©crite dans le fichier de migration.

## Anti DDoS <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Prise en charge de STREAM :x:

Le  plug-in **anti-DDoS** offre une protection avanc√©e contre les attaques par d√©ni de service distribu√© (DDoS) en surveillant, analysant et filtrant le trafic suspect en temps r√©el.

En utilisant un m√©canisme de **fen√™tre glissante**, le plugin maintient un dictionnaire en m√©moire des horodatages des requ√™tes pour d√©tecter les pics de trafic anormaux √† partir d'adresses IP individuelles. En fonction du mode de s√©curit√© configur√©, il peut soit bloquer les connexions incrimin√©es, soit consigner l'activit√© suspecte pour un examen plus approfondi.

### Fonctionnalit√©s

- **Analyse du trafic en temps r√©el :** surveille en permanence les demandes entrantes pour d√©tecter les attaques DDoS potentielles.
- **M√©canisme de fen√™tre glissante :** suit l'activit√© r√©cente des demandes dans une fen√™tre de temps configurable.
- **Seuils configurables :** vous permet de d√©finir le nombre maximum de requ√™tes suspectes par IP.
- **Logique de blocage avanc√©e :** √©value √† la fois le nombre de requ√™tes par IP et le nombre d'adresses IP distinctes d√©passant le seuil.
- **Modes de s√©curit√© flexibles :** choisissez entre le blocage imm√©diat de la connexion ou le mode de d√©tection uniquement (journalisation).
- **Magasin de donn√©es en m√©moire optimis√© :** Garantit des recherches √† grande vitesse et un suivi efficace des m√©triques.
- **Entretien m√©nager automatique :** efface p√©riodiquement les donn√©es obsol√®tes pour maintenir des performances optimales.

### Configuration

Personnalisez le comportement du plug-in √† l'aide des param√®tres suivants :

| R√©glage                      | Faire d√©faut  | Contexte | Multiple | Description                                                                                                 |
| ---------------------------- | ------------- | -------- | -------- | ----------------------------------------------------------------------------------------------------------- |
| `USE_ANTIDDOS`               | `no`          | global   | Non      | Activez ou d√©sactivez la protection anti-DDoS. R√©glez sur `"yes"` pour activer le plugin.                   |
| `ANTIDDOS_METRICS_DICT_SIZE` | `10M`         | global   | Non      | Taille de la banque de donn√©es en m√©moire pour le suivi des m√©triques DDoS (par exemple, `10M`, `500k`).    |
| `ANTIDDOS_THRESHOLD`         | `100`         | global   | Non      | Nombre maximum de requ√™tes suspectes autoris√©es par IP dans la fen√™tre de temps d√©finie.                    |
| `ANTIDDOS_WINDOW_TIME`       | `10`          | global   | Non      | Fen√™tre de temps en secondes pendant laquelle les demandes suspectes sont comptabilis√©es.                   |
| `ANTIDDOS_STATUS_CODES`      | `429 403 444` | global   | Non      | Codes d'√©tat HTTP consid√©r√©s comme suspects et utilis√©s pour d√©clencher des actions anti-DDoS.              |
| `ANTIDDOS_DISTINCT_IP`       | `5`           | global   | Non      | Nombre minimum d'adresses IP distinctes qui doivent d√©passer le seuil avant d'appliquer le mode de blocage. |

### Bonnes pratiques

- **R√©glage du seuil :** ajustez `ANTIDDOS_THRESHOLD` et `ANTIDDOS_WINDOW_TIME` en fonction de vos mod√®les de trafic typiques.
- **R√©vision du code d'√©tat :** mettez r√©guli√®rement √† jour `ANTIDDOS_STATUS_CODES` pour capturer les comportements suspects nouveaux ou en √©volution.
- **Surveillance :** analysez r√©guli√®rement les journaux et les m√©triques pour affiner les param√®tres et am√©liorer la protection globale.

## Gestionnaire d'utilisateurs <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

<p align="center">
    <iframe style="display: block;" width="560" height="315" data-src="https://www.youtube-nocookie.com/embed/EIohiUf9Fg4" title="Gestionnaire d'utilisateurs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

Le plug-in de gestion des utilisateurs offre une interface robuste pour l'administration des comptes d'utilisateurs au sein de votre syst√®me.

Avec ce plugin, les administrateurs peuvent sans effort cr√©er, mettre √† jour et d√©sactiver des comptes utilisateurs, g√©rer les r√¥les des utilisateurs, basculer l'authentification √† deux facteurs (2FA) et afficher des informations d√©taill√©es sur les utilisateurs telles que les horodatages de la derni√®re connexion et les statuts des comptes (actif ou inactif). Con√ßu dans un souci de s√©curit√© et de facilit√© d'utilisation, ce plug-in simplifie les t√¢ches de gestion des utilisateurs tout en garantissant la conformit√© et l'auditabilit√©.

### Fonctionnalit√©s

- **Op√©rations de compte d'utilisateur :** importez au format CSV/XSLX, cr√©ez, modifiez et supprimez des comptes d'utilisateur en toute simplicit√©.
- **Contr√¥le d'acc√®s bas√© sur les r√¥les :** Attribuez et modifiez les r√¥les d'utilisateur pour g√©rer les autorisations et les niveaux d'acc√®s.
- **Gestion 2FA :** d√©sactivez l'authentification √† deux facteurs en fonction des d√©cisions administratives.
- **Informations compl√®tes sur les utilisateurs :** surveillez les donn√©es cl√©s des utilisateurs, notamment les heures de derni√®re connexion, les dates de cr√©ation de compte et le statut actif/inactif.
- **Journalisation des audits :** conservez une piste d'audit pour toutes les actions de gestion des utilisateurs afin d'am√©liorer la s√©curit√© et la conformit√©.

<figure markdown>
  ![Vue d'ensemble](assets/img/user-manager.png){ align=center }
  <figcaption>Page Gestionnaire d'utilisateurs</figcaption>
</figure>

<figure markdown>
  ![Cr√©er un formulaire utilisateur](assets/img/user-manager-create.png){ align=center }
  <figcaption>Gestionnaire d'utilisateurs - Cr√©er un formulaire d'utilisateur</figcaption>
</figure>

<figure markdown>
  ![Page d'activit√©s](assets/img/user-manager-activities.png){ align=center }
  <figcaption>Gestionnaire d'utilisateurs - Page Activit√©s</figcaption>
</figure>

## Easy Resolve <img src='../../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

Le plugin Easy Resolve vous permet de rem√©dier rapidement aux faux positifs et aux probl√®mes r√©currents directement depuis la page Rapports. Il transforme les actions guid√©es "R√©soudre" en mises √† jour de configuration s√ªres et √©tendues‚Äîsans √©dition manuelle.

### Fonctionnalit√©s

- Actions en un clic depuis les Rapports et les d√©tails des rapports.
- Suggestions contextuelles pour ModSecurity, blacklist et DNSBL.
- G√©n√®re des exclusions ModSecurity s√ªres ou met √† jour les listes d'ignorance.
- Applique les changements au niveau du service ou global avec v√©rifications des permissions.
- Ouverture automatique optionnelle de la page de configuration li√©e apr√®s application.

<figure markdown>
  ![Vue d'ensemble](assets/img/easy-resolve.png){ align=center }
  <figcaption>Page Rapports - avec Easy Resolve</figcaption>
</figure>

<div class="grid grid-2" markdown>
<figure markdown>
  ![R√©soudre ModSecurity](assets/img/easy-resolve-modsecurity.png){ width="100%" }
  <figcaption>R√©soudre ModSecurity</figcaption>
</figure>
<figure markdown>
  ![R√©soudre DNSBL](assets/img/easy-resolve-dnsbl.png){ width="100%" }
  <figcaption>R√©soudre DNSBL</figcaption>
</figure>
</div>

<div class="grid grid-5" markdown>
<figure markdown>
  ![R√©soudre Blacklist - IP](assets/img/easy-resolve-blacklist-ip.png){ width="100%" }
  <figcaption>Blacklist - IP</figcaption>
</figure>
<figure markdown>
  ![R√©soudre Blacklist - User‚ÄëAgent](assets/img/easy-resolve-blacklist-ua.png){ width="100%" }
  <figcaption>Blacklist - User‚ÄëAgent</figcaption>
</figure>
<figure markdown>
  ![R√©soudre Blacklist - rDNS](assets/img/easy-resolve-blacklist-rdns.png){ width="100%" }
  <figcaption>Blacklist - rDNS</figcaption>
</figure>
<figure markdown>
  ![R√©soudre Blacklist - ASN](assets/img/easy-resolve-blacklist-asn.png){ width="100%" }
  <figcaption>Blacklist - ASN</figcaption>
</figure>
<figure markdown>
  ![R√©soudre Blacklist - URI](assets/img/easy-resolve-blacklist-uri.png){ width="100%" }
  <figcaption>Blacklist - URI</figcaption>
</figure>
</div>

## Load Balancer <img src='../assets/img/pro-icon.svg' alt='crow pro icon' height='24px' width='24px' style="transform : translateY(3px);"> (PRO)

<p align="center">
    <iframe style="display: block;" width="560" height="315" data-src="https://www.youtube-nocookie.com/embed/cOVp0rAt5nw?si=iVhDio8o8S4F_uag" title="Load Balancer" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

Le plugin Load Balancer transforme BunkerWeb en un directeur de trafic avec garde-fous. D√©clarez des pools upstream une fois, pointez votre proxy inverse vers eux, et laissez le √©quilibrage conscient de la sant√© garder les utilisateurs sur des backends r√©actifs. Le mode sticky cookie √©met automatiquement un cookie `BWLBID` pour que les sessions restent ancr√©es o√π vous le souhaitez.

### Fonctionnalit√©s

- Blocs par upstream : nommez les pools et r√©utilisez-les sur les h√¥tes proxy inverse.
- √âquilibrage flexible : round-robin par d√©faut, ou sticky via IP ou cookie sign√©.
- Cibles intelligentes : r√©solution DNS optionnelle pour les backends hostname plus r√©glage keepalive.
- Sant√© int√©gr√©e : sondes HTTP/HTTPS avec chemins personnalis√©s, intervalles, codes de statut et v√©rifications SSL.
- Continuit√© de session : cookie `BWLBID` automatique lorsque le mode sticky-cookie est activ√©.

### Configuration

**D√©finition upstream**

| Param√®tre                                 | D√©faut        | Contexte | Multiple | Description                                                                       |
| ----------------------------------------- | ------------- | -------- | -------- | --------------------------------------------------------------------------------- |
| `LOADBALANCER_UPSTREAM_NAME`              |               | global   | oui      | Identifiant upstream (r√©f√©renc√© par `REVERSE_PROXY_HOST`).                        |
| `LOADBALANCER_UPSTREAM_SERVERS`           |               | global   | oui      | Liste s√©par√©e par espaces d'adresses backend (ex. `10.0.0.1:8080 10.0.0.2:8080`). |
| `LOADBALANCER_UPSTREAM_MODE`              | `round-robin` | global   | oui      | Strat√©gie d'√©quilibrage (`round-robin` ou `sticky`).                              |
| `LOADBALANCER_UPSTREAM_STICKY_METHOD`     | `ip`          | global   | oui      | M√©thode sticky (`ip` ou `cookie`). Mode cookie √©met `BWLBID`.                     |
| `LOADBALANCER_UPSTREAM_RESOLVE`           | `no`          | global   | oui      | R√©soudre les hostnames upstream via DNS.                                          |
| `LOADBALANCER_UPSTREAM_KEEPALIVE`         |               | global   | oui      | Connexions keepalive par worker.                                                  |
| `LOADBALANCER_UPSTREAM_KEEPALIVE_TIMEOUT` | `60s`         | global   | oui      | Timeout inactif pour les connexions keepalive.                                    |
| `LOADBALANCER_UPSTREAM_KEEPALIVE_TIME`    | `1h`          | global   | oui      | Dur√©e de vie maximale pour les connexions keepalive.                              |

**V√©rifications de sant√©**

| Param√®tre                                 | D√©faut    | Contexte | Multiple | Description                                                            |
| ----------------------------------------- | --------- | -------- | -------- | ---------------------------------------------------------------------- |
| `LOADBALANCER_HEALTHCHECK_DICT_SIZE`      | `10m`     | global   | non      | Taille du dictionnaire partag√© pour l'√©tat des v√©rifications de sant√©. |
| `LOADBALANCER_HEALTHCHECK_URL`            | `/status` | global   | oui      | Chemin √† sonder sur chaque backend.                                    |
| `LOADBALANCER_HEALTHCHECK_INTERVAL`       | `2000`    | global   | oui      | Intervalle entre v√©rifications (ms).                                   |
| `LOADBALANCER_HEALTHCHECK_TIMEOUT`        | `1000`    | global   | oui      | Timeout par v√©rification (ms).                                         |
| `LOADBALANCER_HEALTHCHECK_FALL`           | `3`       | global   | oui      | √âchecs cons√©cutifs avant de marquer comme down.                        |
| `LOADBALANCER_HEALTHCHECK_RISE`           | `1`       | global   | oui      | Succ√®s cons√©cutifs avant de marquer comme up.                          |
| `LOADBALANCER_HEALTHCHECK_VALID_STATUSES` | `200`     | global   | oui      | Liste s√©par√©e par espaces de codes de statut HTTP valides.             |
| `LOADBALANCER_HEALTHCHECK_CONCURRENCY`    | `10`      | global   | oui      | Maximum de sondes concurrentes.                                        |
| `LOADBALANCER_HEALTHCHECK_TYPE`           | `http`    | global   | oui      | Protocole pour les v√©rifications de sant√© (`http` ou `https`).         |
| `LOADBALANCER_HEALTHCHECK_SSL_VERIFY`     | `yes`     | global   | oui      | V√©rifier les certificats TLS lors des v√©rifications HTTPS.             |
| `LOADBALANCER_HEALTHCHECK_HOST`           |           | global   | oui      | Remplacer l'en-t√™te Host pendant les v√©rifications (utile pour SNI).   |

### D√©marrage rapide

1. D√©finissez votre pool : d√©finissez `LOADBALANCER_UPSTREAM_NAME=my-app` et listez les cibles dans `LOADBALANCER_UPSTREAM_SERVERS` (ex. `10.0.0.1:8080 10.0.0.2:8080`).
2. Dirigez le trafic : d√©finissez `REVERSE_PROXY_HOST=http://my-app` pour que le proxy inverse utilise l'upstream nomm√©.
3. Choisissez un mode : gardez round-robin par d√©faut ou d√©finissez `LOADBALANCER_UPSTREAM_MODE=sticky` avec `LOADBALANCER_UPSTREAM_STICKY_METHOD=cookie` ou `ip`.
4. Ajoutez de la sant√© : gardez `/status` ou ajustez les URLs, intervalles et statuts valides pour refl√©ter le comportement de votre app.
5. R√©glez les connexions : configurez les valeurs keepalive pour r√©utiliser les connexions backend et r√©duire la surcharge de handshake.

### Conseils d'utilisation

- Faites correspondre `REVERSE_PROXY_HOST` √† `LOADBALANCER_UPSTREAM_NAME` lors de l'utilisation de cookies sticky pour que les clients s'√©pinglent au bon pool.
- Gardez les intervalles et timeouts des v√©rifications de sant√© √©quilibr√©s pour √©viter les oscillations sur les liens lents.
- Activez `LOADBALANCER_UPSTREAM_RESOLVE` lorsque vous pointez vers des hostnames qui peuvent changer via DNS.
- R√©glez les valeurs keepalive pour refl√©ter la capacit√© backend et les objectifs de r√©utilisation des connexions.
